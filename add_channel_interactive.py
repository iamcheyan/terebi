#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTubeé¢‘é“æ·»åŠ å·¥å…· - æ”¯æŒæ‰¹é‡æ·»åŠ 
ç”¨æ³•ï¼š
- äº¤äº’æ¨¡å¼ï¼špython add_channel_interactive.py
- å•ä¸ªé¢‘é“ï¼špython add_channel_interactive.py --url @handle
- æ‰¹é‡æ·»åŠ ï¼špython add_channel_interactive.py --url @handle1 @handle2 @handle3
"""

import argparse
import json
import os
import sys
import urllib.parse
import urllib.request
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Tuple

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent
CONFIG_FILE = PROJECT_ROOT / "japan_tv_youtube_channels.json"
DATA_DIR = PROJECT_ROOT / "data"
IMG_DIR = PROJECT_ROOT / "img"
IMG_RESIZED_DIR = IMG_DIR / "resized"

# å°è¯•å¯¼å…¥ Pillow
try:
    from PIL import Image
    from io import BytesIO
except ImportError:
    Image = None
    BytesIO = None


def load_config() -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not CONFIG_FILE.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_FILE}")
        sys.exit(1)
    
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_config(config: dict) -> bool:
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    try:
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
        return False


def load_api_keys() -> List[str]:
    """ä» WEB-INF/config.properties è¯»å– YouTube API Key"""
    config_file = PROJECT_ROOT / "WEB-INF" / "config.properties"
    if not config_file.exists():
        return []
    
    keys = []
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("youtube.apikey") and "=" in line:
                    key = line.split("=", 1)[1].strip()
                    if key:
                        keys.append(key)
    except Exception:
        pass
    
    return keys


def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ–URLæ ¼å¼"""
    if not url:
        return ""
    
    # å¦‚æœåªæ˜¯handleï¼ˆå¦‚ @handleï¼‰ï¼Œè¡¥å……å®Œæ•´URL
    if url.startswith("@"):
        return f"https://www.youtube.com/{url}"
    
    # ç¡®ä¿æ˜¯å®Œæ•´çš„YouTube URL
    if not url.startswith("http"):
        if url.startswith("www.youtube.com"):
            url = "https://" + url
        elif url.startswith("youtube.com"):
            url = "https://www." + url
        else:
            url = "https://www.youtube.com/" + url
    
    return url


def extract_handle_or_id(url: str) -> str:
    """ä»URLæå–handleæˆ–ID"""
    if not url:
        return ""
    
    # å¤„ç† @handle æ ¼å¼
    if "/@" in url:
        return url.split("/@")[-1].split("?")[0].split("/")[0]
    
    # å¤„ç† /channel/UC... æ ¼å¼
    if "/channel/" in url:
        return url.split("/channel/")[-1].split("?")[0].split("/")[0]
    
    # å¤„ç† /c/channel_name æ ¼å¼
    if "/c/" in url:
        return url.split("/c/")[-1].split("?")[0].split("/")[0]
    
    return ""


def check_channel_exists(url: str) -> bool:
    """æ£€æŸ¥é¢‘é“æ˜¯å¦å·²å­˜åœ¨"""
    config = load_config()
    
    for cat_data in config.values():
        if not isinstance(cat_data, dict):
            continue
        for subcat_data in cat_data.values():
            if not isinstance(subcat_data, list):
                continue
            for channel in subcat_data:
                if isinstance(channel, dict) and channel.get("url") == url:
                    return True
    return False


def upsert_channel(url: str, name: str, category: str, subcategory: str) -> bool:
    """æ·»åŠ æˆ–æ›´æ–°é¢‘é“åˆ°é…ç½®"""
    config = load_config()
    
    # æŸ¥æ‰¾ç°æœ‰é¢‘é“
    for cat_data in config.values():
        if not isinstance(cat_data, dict):
            continue
        for subcat_data in cat_data.values():
            if not isinstance(subcat_data, list):
                continue
            for i, channel in enumerate(subcat_data):
                if isinstance(channel, dict) and channel.get("url") == url:
                    # æ›´æ–°ç°æœ‰é¢‘é“
                    subcat_data[i] = {
                        "name": name,
                        "url": url,
                        "bakname": "",
                        "cached": False,
                        "skip": False
                    }
                    print(f"âœ… å·²æ›´æ–°é¢‘é“: {name}")
                    return save_config(config)
    
    # æ·»åŠ æ–°é¢‘é“
    if category not in config:
        config[category] = {}
    if subcategory not in config[category]:
        config[category][subcategory] = []
    
    new_channel = {
        "name": name,
        "url": url,
        "bakname": "",
        "cached": False,
        "skip": False
    }
    
    config[category][subcategory].append(new_channel)
    print(f"âœ… å·²æ·»åŠ é¢‘é“: {name}")
    return save_config(config)


def http_get_json(url: str, params: dict) -> dict:
    """å‘é€HTTP GETè¯·æ±‚å¹¶è¿”å›JSON"""
    if params:
        query = urllib.parse.urlencode(params)
        url = f"{url}?{query}"
    
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except Exception as e:
        print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")
        return {}


def resolve_channel_id(url: str, api_key: str) -> Tuple[Optional[str], Optional[str]]:
    """è§£æé¢‘é“IDå’Œæ ‡é¢˜"""
    handle_or_id = extract_handle_or_id(url)
    if not handle_or_id:
        return None, None
    
    # å°è¯•ä½œä¸ºhandleæŸ¥è¯¢
    if not handle_or_id.startswith("UC"):
        try:
            # è§£ç ç™¾åˆ†å·ç¼–ç 
            decoded_handle = urllib.parse.unquote(handle_or_id)
            resp = http_get_json(
                "https://www.googleapis.com/youtube/v3/channels",
                {
                    "part": "snippet",
                    "forUsername": decoded_handle,
                    "key": api_key
                }
            )
            items = resp.get("items", [])
            if items:
                return items[0]["id"], items[0]["snippet"]["title"]
        except Exception:
            pass
    
    # å°è¯•ä½œä¸ºIDæŸ¥è¯¢
    try:
        resp = http_get_json(
            "https://www.googleapis.com/youtube/v3/channels",
            {
                "part": "snippet",
                "id": handle_or_id,
                "key": api_key
            }
        )
        items = resp.get("items", [])
        if items:
            return items[0]["id"], items[0]["snippet"]["title"]
    except Exception:
        pass
    
    return None, None


def resolve_channel_id_via_html(url: str) -> Tuple[Optional[str], Optional[str]]:
    """åœ¨æ— APIæƒ…å†µä¸‹ï¼Œé€šè¿‡æŠ“å–é¢‘é“é¡µHTMLæå– channelId å’Œæ ‡é¢˜ã€‚
    é€‚é… @handle ä¸ /channel/UC... é“¾æ¥ã€‚
    """
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"âŒ HTMLæŠ“å–å¤±è´¥: {e}")
        return None, None

    # ç›´æ¥ä»HTMLä¸­åŒ¹é… channelId
    m = re.search(r'"channelId"\s*:\s*"(UC[\w-]{22})"', html)
    channel_id = m.group(1) if m else None

    # æŠ“å–æ ‡é¢˜ï¼ˆä¼˜å…ˆ og:titleï¼Œå…¶æ¬¡ <title>ï¼‰
    title = None
    m_title = re.search(r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\']([^"\']+)["\']', html)
    if m_title:
        title = m_title.group(1).strip()
    else:
        m_title2 = re.search(r"<title>(.*?)</title>", html, re.S)
        if m_title2:
            title = re.sub(r"\s+\-\s+YouTube$", "", m_title2.group(1).strip())

    return channel_id, title


def fetch_channel_uploads_via_rss(channel_id: str, max_count: int = 200) -> List[dict]:
    """æ— éœ€APIï¼Œä½¿ç”¨YouTubeå®˜æ–¹RSSè·å–è§†é¢‘åˆ—è¡¨ã€‚
    å‚è€ƒ: https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}
    """
    if not channel_id:
        return []
    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        req = urllib.request.Request(feed_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            xml_text = resp.read()
    except Exception as e:
        print(f"âŒ RSSè·å–å¤±è´¥: {e}")
        return []

    try:
        root = ET.fromstring(xml_text)
    except Exception as e:
        print(f"âŒ RSSè§£æå¤±è´¥: {e}")
        return []

    ns = {
        'atom': 'http://www.w3.org/2005/Atom',
        'media': 'http://search.yahoo.com/mrss/'
    }
    entries = root.findall('atom:entry', ns)

    videos: List[dict] = []
    for entry in entries[:max_count]:
        video_id = (entry.find('yt:videoId', {'yt': 'http://www.youtube.com/xml/schemas/2015'}) or {}).text if entry is not None else None
        if not video_id:
            # å¤‡ç”¨ï¼šä» link href ä¸­è§£æ v å‚æ•°
            link_el = entry.find('atom:link', ns)
            href = link_el.get('href') if link_el is not None else ''
            q = urllib.parse.urlparse(href).query
            qs = urllib.parse.parse_qs(q)
            video_id = (qs.get('v') or [''])[0]

        title_el = entry.find('atom:title', ns)
        published_el = entry.find('atom:published', ns)
        media_group = entry.find('media:group', ns)
        thumb_url = None
        if media_group is not None:
            thumb = media_group.find('media:thumbnail', ns)
            if thumb is not None:
                thumb_url = thumb.get('url')

        videos.append({
            "id": video_id or "",
            "title": (title_el.text if title_el is not None else ""),
            "description": "",
            "publishedAt": (published_el.text if published_el is not None else ""),
            "thumbnails": {"default": {"url": thumb_url}} if thumb_url else {},
            "url": f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
        })

    return videos


def fetch_channel_uploads(channel_id: str, api_key: str, max_count: int = 200) -> List[dict]:
    """è·å–é¢‘é“ä¸Šä¼ çš„è§†é¢‘åˆ—è¡¨"""
    videos = []
    next_page_token = None
    
    while len(videos) < max_count:
        params = {
            "part": "snippet",
            "channelId": channel_id,
            "type": "video",
            "order": "date",
            "maxResults": min(50, max_count - len(videos)),
            "key": api_key
        }
        
        if next_page_token:
            params["pageToken"] = next_page_token
        
        resp = http_get_json("https://www.googleapis.com/youtube/v3/search", params)
        items = resp.get("items", [])
        
        if not items:
            break
        
        for item in items:
            snippet = item.get("snippet", {})
            video = {
                "id": item.get("id", {}).get("videoId", ""),
                "title": snippet.get("title", ""),
                "description": snippet.get("description", ""),
                "publishedAt": snippet.get("publishedAt", ""),
                "thumbnails": snippet.get("thumbnails", {}),
                "url": f"https://www.youtube.com/watch?v={item.get('id', {}).get('videoId', '')}"
            }
            videos.append(video)
        
        next_page_token = resp.get("nextPageToken")
        if not next_page_token:
            break
    
    return videos[:max_count]


def save_data_file(name: str, channel_id: str, channel_title: str, videos: List[dict]) -> Path:
    """ä¿å­˜è§†é¢‘æ•°æ®åˆ°JSONæ–‡ä»¶"""
    DATA_DIR.mkdir(exist_ok=True)
    
    payload = {
        "channel_id": channel_id,
        "channel_name": channel_title or name,
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "videos": videos,
    }
    
    out_path = DATA_DIR / f"{name}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    
    return out_path


def pick_best_thumbnail(snippet: dict) -> Optional[str]:
    """é€‰æ‹©æœ€ä½³ç¼©ç•¥å›¾URL"""
    thumbs = (snippet or {}).get("thumbnails", {})
    for key in ["maxres", "standard", "high", "medium", "default"]:
        url = (thumbs.get(key) or {}).get("url")
        if url:
            return url
    return None


def download_channel_avatar(channel_id: str, api_key: str, save_name: str) -> Tuple[Optional[Path], Optional[Path]]:
    """ä¸‹è½½é¢‘é“å¤´åƒå¹¶ç”Ÿæˆç¼©ç•¥å›¾ï¼Œè¿”å› (åŸå›¾è·¯å¾„, ç¼©ç•¥å›¾è·¯å¾„)"""
    # æ‹‰å–é¢‘é“ä¿¡æ¯ä»¥è·å–ç¼©ç•¥å›¾
    ch = http_get_json(
        "https://www.googleapis.com/youtube/v3/channels",
        {"part": "snippet", "id": channel_id, "key": api_key}
    )
    try:
        snippet = ch.get("items", [])[0]["snippet"]
    except Exception:
        snippet = None
    img_url = pick_best_thumbnail(snippet) if snippet else None
    if not img_url:
        return None, None

    # ä¸‹è½½å›¾ç‰‡
    try:
        req = urllib.request.Request(img_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=20) as resp:
            content = resp.read()
    except Exception:
        return None, None

    IMG_DIR.mkdir(parents=True, exist_ok=True)
    IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
    raw_path = IMG_DIR / f"{save_name}.jpg"
    resized_path = IMG_RESIZED_DIR / f"{save_name}.jpg"

    # ä¿å­˜åŸå›¾
    try:
        with open(raw_path, "wb") as f:
            f.write(content)
    except Exception:
        raw_path = None

    # ç”Ÿæˆç¼©ç•¥å›¾
    try:
        if Image is None:
            return raw_path, None
        with Image.open(BytesIO(content)) as im:
            # è½¬æˆRGBï¼Œç­‰æ¯”ç¼©æ”¾åˆ° 128x128 ç”»å¸ƒå†…ï¼Œå†å±…ä¸­é“ºæ»¡è£å‰ªï¼ˆæ–¹å½¢ï¼‰
            im = im.convert("RGB")
            size = 128
            # å…ˆæŒ‰çŸ­è¾¹ç­‰æ¯”æ”¾å¤§ï¼Œåå±…ä¸­è£å‰ª
            ratio = max(size / im.width, size / im.height)
            new_w, new_h = int(im.width * ratio), int(im.height * ratio)
            im = im.resize((new_w, new_h), Image.LANCZOS)
            left = (new_w - size) // 2
            top = (new_h - size) // 2
            im = im.crop((left, top, left + size, top + size))
            im.save(resized_path, format="JPEG", quality=88, optimize=True)
    except Exception:
        resized_path = None

    return raw_path, resized_path


def process_single_channel(url: str, name: str, category: str, subcategory: str) -> bool:
    """å¤„ç†å•ä¸ªé¢‘é“ï¼šæ·»åŠ åˆ°é…ç½®å¹¶æŠ“å–æ•°æ®"""
    url = normalize_url(url)
    if not url:
        print(f"âŒ URL æ— æ•ˆ: {url}")
        return False

    # é»˜è®¤åç§°ï¼šä»URLæå– handle/IDï¼Œå¹¶å¯¹ç™¾åˆ†å·ç¼–ç è¿›è¡Œè§£ç 
    if not name:
        name = urllib.parse.unquote(extract_handle_or_id(url)).strip()
        if not name:
            print(f"âŒ æ— æ³•ç¡®å®šé¢‘é“åç§°: {url}")
            return False

    print(f"=== å¤„ç†é¢‘é“: {name} ===")
    print(f"URL: {url}")
    print(f"åˆ†ç±»/å­åˆ†ç±»: {category} / {subcategory}")

    # æ£€æŸ¥é¢‘é“æ˜¯å¦å·²å­˜åœ¨
    if check_channel_exists(url):
        print(f"â­ï¸ é¢‘é“å·²å­˜åœ¨ï¼Œè·³è¿‡: {name}")
        # å¦‚æœå·²å­˜åœ¨ä½†ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªç©ºçš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…å‰ç«¯404
        data_path = DATA_DIR / f"{name}.json"
        if not data_path.exists():
            try:
                empty = {
                    "channel_id": "",
                    "channel_name": name,
                    "updated_at": datetime.now(timezone.utc).isoformat(),
                    "videos": []
                }
                DATA_DIR.mkdir(exist_ok=True)
                with open(data_path, "w", encoding="utf-8") as f:
                    json.dump(empty, f, ensure_ascii=False, indent=2)
                print(f"ğŸ§© å·²è¡¥é½ç©ºæ•°æ®æ–‡ä»¶: {data_path}")
            except Exception:
                pass
        # åŒæ—¶ä¿è¯æœ‰å ä½å¤´åƒ
        placeholder = IMG_RESIZED_DIR / "placeholder.jpg"
        target_logo = IMG_RESIZED_DIR / f"{name}.jpg"
        try:
            IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
            if placeholder.exists() and not target_logo.exists():
                import shutil
                shutil.copyfile(placeholder, target_logo)
                print(f"ğŸ§© å·²å¤åˆ¶å ä½å¤´åƒ: {target_logo}")
        except Exception:
            pass
        return True

    # æ·»åŠ åˆ°é…ç½®
    ok = upsert_channel(url=url, name=name, category=category, subcategory=subcategory)
    if not ok:
        print(f"âŒ æ·»åŠ å¤±è´¥: {name}")
        return False

    print(f"âœ… å·²æ·»åŠ åˆ°é…ç½®: {name}")

    # æŠ“å–è¯¥URLå¯¹åº”é¢‘é“å¹¶ç”Ÿæˆ data/{åç§°}.json
    print("=== æ­£åœ¨è¯»å– API Key å¹¶æŠ“å–è¯¥é¢‘é“ï¼ˆå¸¦HTML/RSSå›é€€ï¼‰ ===")
    keys = load_api_keys()
    # å…ˆå°è¯•æ— éœ€APIçš„HTMLè§£æè·å– channelId
    ch_id_html, ch_title_html = resolve_channel_id_via_html(url)
    if ch_id_html:
        # æ— APIï¼šç”¨RSSæŠ“å–è§†é¢‘
        videos = fetch_channel_uploads_via_rss(ch_id_html, max_count=200)
        out_path = save_data_file(name=name, channel_id=ch_id_html, channel_title=ch_title_html or name, videos=videos)
        # å¤´åƒï¼šå°è¯•ä»HTMLæå– og:image
        try:
            req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urllib.request.urlopen(req, timeout=20) as resp:
                html = resp.read().decode("utf-8", errors="ignore")
            m_img = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html)
            if m_img and Image is not None:
                # ä¸‹è½½å¹¶ç”Ÿæˆç¼©ç•¥å›¾
                try:
                    req2 = urllib.request.Request(m_img.group(1), headers={"User-Agent": "Mozilla/5.0"})
                    with urllib.request.urlopen(req2, timeout=20) as resp2:
                        content = resp2.read()
                    IMG_DIR.mkdir(parents=True, exist_ok=True)
                    IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
                    raw_path = IMG_DIR / f"{name}.jpg"
                    with open(raw_path, "wb") as f:
                        f.write(content)
                    with Image.open(BytesIO(content)) as im:
                        im = im.convert("RGB")
                        size = 128
                        ratio = max(size / im.width, size / im.height)
                        new_w, new_h = int(im.width * ratio), int(im.height * ratio)
                        im = im.resize((new_w, new_h), Image.LANCZOS)
                        left = (new_w - size) // 2
                        top = (new_h - size) // 2
                        im = im.crop((left, top, left + size, top + size))
                        im.save(IMG_RESIZED_DIR / f"{name}.jpg", format="JPEG", quality=88, optimize=True)
                except Exception:
                    pass
        except Exception:
            pass
        print(f"âœ… æŠ“å–å®Œæˆï¼ˆRSSï¼‰ï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
        return True

    if not keys:
        print("âŒ æœªæ‰¾åˆ° API Keyï¼Œä¸”HTMLè§£æå¤±è´¥ã€‚ä»…å®Œæˆæ·»åŠ åˆ°é…ç½®å¹¶ç”Ÿæˆç©ºæ•°æ®ã€‚")
        out_path = save_data_file(name=name, channel_id="", channel_title=name, videos=[])
        print(f"ğŸ§© å·²ç”Ÿæˆç©ºæ•°æ®æ–‡ä»¶ï¼š{out_path}")
        # å ä½å¤´åƒ
        try:
            placeholder = IMG_RESIZED_DIR / "placeholder.jpg"
            target_logo = IMG_RESIZED_DIR / f"{name}.jpg"
            IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
            if placeholder.exists() and not target_logo.exists():
                import shutil
                shutil.copyfile(placeholder, target_logo)
        except Exception:
            pass
        return True

    # è½®æ¢ API Key è§£æé¢‘é“ID
    ch_id, ch_title = None, None
    api_key = None
    for k in keys:
        api_key = k
        ch_id, ch_title = resolve_channel_id(url, k)
        if ch_id:
            break
    if not ch_id:
        # å†æ¬¡å°è¯•HTMLè§£æ + RSSï¼ˆAPIå¤±è´¥å¯èƒ½403/é…é¢/é™åˆ¶ï¼‰
        ch_id_html, ch_title_html = resolve_channel_id_via_html(url)
        if ch_id_html:
            videos = fetch_channel_uploads_via_rss(ch_id_html, max_count=200)
            out_path = save_data_file(name=name, channel_id=ch_id_html, channel_title=ch_title_html or name, videos=videos)
            print(f"âœ… æŠ“å–å®Œæˆï¼ˆRSSï¼‰ï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
            return True
        print("âŒ æ— æ³•è§£æé¢‘é“IDï¼ŒæŠ“å–ç»ˆæ­¢ã€‚å·²å®Œæˆæ·»åŠ åˆ°é…ç½®ã€‚")
        # ä»ç„¶ç”Ÿæˆç©ºçš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…å‰ç«¯404
        out_path = save_data_file(name=name, channel_id="", channel_title=name, videos=[])
        print(f"ğŸ§© å·²ç”Ÿæˆç©ºæ•°æ®æ–‡ä»¶ï¼š{out_path}")
        # å ä½å¤´åƒ
        try:
            placeholder = IMG_RESIZED_DIR / "placeholder.jpg"
            target_logo = IMG_RESIZED_DIR / f"{name}.jpg"
            IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
            if placeholder.exists() and not target_logo.exists():
                import shutil
                shutil.copyfile(placeholder, target_logo)
        except Exception:
            pass
        return True

    videos = fetch_channel_uploads(ch_id, api_key, max_count=200)
    out_path = save_data_file(name=name, channel_id=ch_id, channel_title=ch_title or name, videos=videos)

    # ä¸‹è½½å¹¶ç”Ÿæˆå¤´åƒç¼©ç•¥å›¾ï¼ˆä¸ä¸­æ–­ä¸»æµç¨‹ï¼‰
    raw_img, resized_img = download_channel_avatar(channel_id=ch_id, api_key=api_key, save_name=name)

    print(f"âœ… æŠ“å–å®Œæˆï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
    if raw_img:
        print(f"âœ… å·²ä¸‹è½½å¤´åƒï¼š{raw_img}")
    if resized_img:
        print(f"âœ… å·²ç”Ÿæˆç¼©ç•¥å›¾ï¼š{resized_img}")
    else:
        print("âš ï¸ æœªç”Ÿæˆç¼©ç•¥å›¾ï¼ˆå¯èƒ½æœªå®‰è£… Pillow æˆ–ä¸‹è½½å¤±è´¥ï¼‰")
    
    return True


def main():
    parser = argparse.ArgumentParser(description="æ·»åŠ YouTubeé¢‘é“åˆ°é…ç½®å¹¶æŠ“å–æ•°æ®")
    parser.add_argument("--url", nargs="+", help="YouTubeé¢‘é“URLï¼ˆæ”¯æŒå¤šä¸ªï¼Œå¦‚ --url @handle1 @handle2ï¼‰")
    parser.add_argument("--name", help="æ˜¾ç¤ºåç§°ï¼ˆé»˜è®¤ç”¨ handle/IDï¼‰")
    parser.add_argument("--category", default="ãã®ä»–", help="åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ï¼‰")
    parser.add_argument("--subcategory", default="ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«", help="å­åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰")
    parser.add_argument("--yes", "-y", action="store_true", help="è‡ªåŠ¨ç¡®è®¤ï¼Œä¸è¯¢é—®")
    args = parser.parse_args()

    if args.url:
        # æ‰¹é‡æ·»åŠ æ¨¡å¼
        urls = args.url
        category = args.category
        subcategory = args.subcategory
        
        print(f"=== æ‰¹é‡æ·»åŠ æ¨¡å¼ ===")
        print(f"é¢‘é“æ•°é‡: {len(urls)}")
        print(f"åˆ†ç±»/å­åˆ†ç±»: {category} / {subcategory}")
        print(f"URLåˆ—è¡¨:")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        if not args.yes:
            try:
                if input(f"\nç¡®è®¤æ‰¹é‡æ·»åŠ  {len(urls)} ä¸ªé¢‘é“ï¼Ÿ(y/N): ").lower() != 'y':
                    print("å·²å–æ¶ˆ")
                    return
            except EOFError:
                print("âŒ æ— æ³•è¯»å–è¾“å…¥ï¼Œè¯·ä½¿ç”¨ --yes å‚æ•°è‡ªåŠ¨ç¡®è®¤")
                return
        
        success_count = 0
        for i, url in enumerate(urls, 1):
            print(f"\n=== å¤„ç†ç¬¬ {i}/{len(urls)} ä¸ªé¢‘é“ ===")
            try:
                if process_single_channel(url, args.name, category, subcategory):
                    success_count += 1
                    print(f"âœ… ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å®Œæˆ")
                else:
                    print(f"âŒ ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å¤±è´¥")
            except Exception as e:
                print(f"âŒ ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"\n=== æ‰¹é‡å¤„ç†å®Œæˆ ===")
        print(f"æˆåŠŸ: {success_count}/{len(urls)} ä¸ªé¢‘é“")
        if success_count > 0:
            print("ğŸ‘‰ è¯·åˆ·æ–°ç½‘é¡µæŸ¥çœ‹æ–°é¢‘é“")
    else:
        # äº¤äº’æ¨¡å¼
        try:
            url = input("è¯·è¾“å…¥YouTubeé¢‘é“åœ°å€: ").strip()
        except EOFError:
            print("âŒ æœªæä¾›URL")
            sys.exit(1)

        if not url:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
            return

        name = args.name
        if not name:
            name = urllib.parse.unquote(extract_handle_or_id(url)).strip()
            if not name:
                print("âŒ æ— æ³•ç¡®å®šé¢‘é“åç§°ï¼Œè¯·ä½¿ç”¨ --name æŒ‡å®š")
                sys.exit(1)

        print("=== é¢„è§ˆ ===")
        print(f"URL: {url}")
        print(f"åç§°: {name}")
        print(f"åˆ†ç±»/å­åˆ†ç±»: {args.category} / {args.subcategory}")

        if input("\nç¡®è®¤æ·»åŠ ï¼Ÿ(y/N): ").lower() != 'y':
            print("å·²å–æ¶ˆ")
            return

        process_single_channel(url, name, args.category, args.subcategory)


if __name__ == "__main__":
    main()