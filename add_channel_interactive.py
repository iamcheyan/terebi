#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import json
import re
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Tuple
import urllib.parse
import urllib.request
from io import BytesIO

try:
    from PIL import Image  # å¤´åƒç¼©æ”¾
except Exception:
    Image = None


PROJECT_ROOT = Path(__file__).resolve().parent
CHANNELS_FILE = PROJECT_ROOT / "japan_tv_youtube_channels.json"
CONFIG_FILE = PROJECT_ROOT / "WEB-INF" / "config.properties"
DATA_DIR = PROJECT_ROOT / "data"
IMG_DIR = PROJECT_ROOT / "img"
IMG_RESIZED_DIR = PROJECT_ROOT / "img" / "resized"


def normalize_url(url: str) -> str:
    url = (url or "").strip()
    if not url:
        return url
    if not url.startswith("http"):
        url = "https://" + url
    return url


def extract_handle_or_id(url: str) -> str:
    try:
        m = re.search(r"youtube\.com/(?:@|channel/|user/|c/)([^/?#]+)", url)
        if m:
            ident = m.group(1)
            return urllib.parse.unquote(ident)
        return url.rstrip("/").split("/")[-1]
    except Exception:
        return url


def ensure_category(channels_data: dict, category: str, subcategory: str) -> None:
    if category not in channels_data:
        channels_data[category] = {}
    if subcategory not in channels_data[category]:
        channels_data[category][subcategory] = []


def channel_exists(channels_data: dict, url: str) -> bool:
    for group in channels_data.values():
        if isinstance(group, dict):
            for lst in group.values():
                if isinstance(lst, list):
                    for ch in lst:
                        if ch.get("url") == url:
                            return True
    return False


def upsert_channel(url: str, name: str, category: str, subcategory: str) -> bool:
    if not CHANNELS_FILE.exists():
        print(f"âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {CHANNELS_FILE}")
        return False

    with open(CHANNELS_FILE, "r", encoding="utf-8") as f:
        channels_data = json.load(f)

    # å°è¯•è¦†ç›–ï¼ˆæŒ‰ URL åŒ¹é…å¹¶æ›´æ–° name/cached å­—æ®µï¼‰
    replaced = False
    for group_key, group in channels_data.items():
        if not isinstance(group, dict):
            continue
        for sub_key, lst in group.items():
            if not isinstance(lst, list):
                continue
            for ch in lst:
                if ch.get("url") == url:
                    ch["name"] = name
                    ch["cached"] = False
                    replaced = True
                    break
            if replaced:
                break
        if replaced:
            break

    ensure_category(channels_data, category, subcategory)

    if not replaced:
        record = {
            "name": name,
            "url": url,
            "cached": False
            # ä¸è®¾ç½® skipï¼Œç¡®ä¿åç»­æŠ“å–æ—¶ä¼šè¢«å¤„ç†
        }
        channels = channels_data[category][subcategory]
        channels.append(record)

    with open(CHANNELS_FILE, "w", encoding="utf-8") as f:
        json.dump(channels_data, f, ensure_ascii=False, indent=4)

    if replaced:
        print(f"âœ… å·²è¦†ç›–é¢‘é“: åç§°='{name}', URL='{url}'ï¼ˆä¿æŒåœ¨åŸåˆ†ç±»/å­åˆ†ç±»ä½ç½®ï¼‰")
    else:
        print(f"âœ… å·²æ·»åŠ é¢‘é“: åç§°='{name}', URL='{url}', åˆ†ç±»='{category}', å­åˆ†ç±»='{subcategory}'")
    return True


# ========================== å•é¢‘é“æŠ“å–ï¼ˆæ— éœ€ä¾èµ–å…¶ä»–è„šæœ¬ï¼‰ ==========================

def load_api_keys() -> List[str]:
    keys: List[str] = []
    if not CONFIG_FILE.exists():
        return keys
    try:
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("youtube.apikey"):
                    parts = line.split("=", 1)
                    if len(parts) == 2:
                        val = parts[1].strip()
                        if val:
                            keys.append(val)
    except Exception:
        pass
    return keys


def http_get_json(url: str, params: dict) -> Optional[dict]:
    full = url + "?" + urllib.parse.urlencode(params)
    req = urllib.request.Request(full, headers={
        "User-Agent": "Mozilla/5.0 (compatible; TerebiBot/1.0)"
    })
    with urllib.request.urlopen(req, timeout=20) as resp:
        data = resp.read()
        return json.loads(data.decode("utf-8"))


def resolve_channel_id(url: str, api_key: str) -> Tuple[Optional[str], Optional[str]]:
    """è¿”å› (channel_id, channel_title)ã€‚"""
    # ç›´æ¥åŒ…å« UC å¼€å¤´
    m = re.search(r"youtube\.com/channel/(UC[\w-]{20,})", url)
    if m:
        channel_id = m.group(1)
        # è·å–æ ‡é¢˜
        ch = http_get_json(
            "https://www.googleapis.com/youtube/v3/channels",
            {"part": "snippet", "id": channel_id, "key": api_key}
        )
        title = None
        try:
            title = ch.get("items", [])[0]["snippet"]["title"]
        except Exception:
            title = None
        return channel_id, title

    # handle æˆ– user/c ç»Ÿä¸€ç”¨ search
    identifier = extract_handle_or_id(url)
    sr = http_get_json(
        "https://www.googleapis.com/youtube/v3/search",
        {
            "part": "snippet",
            "q": identifier,
            "type": "channel",
            "maxResults": 5,
            "key": api_key,
        }
    )
    if not sr or not sr.get("items"):
        return None, None
    item = sr["items"][0]
    channel_id = item.get("id", {}).get("channelId")
    channel_title = item.get("snippet", {}).get("title")
    return channel_id, channel_title


def fetch_channel_uploads(channel_id: str, api_key: str, max_count: int = 200) -> List[dict]:
    playlist_id = "UU" + channel_id[2:]
    url = "https://www.googleapis.com/youtube/v3/playlistItems"
    page_token = None
    results: List[dict] = []
    while True:
        params = {
            "part": "snippet",
            "playlistId": playlist_id,
            "maxResults": 50,
            "key": api_key,
        }
        if page_token:
            params["pageToken"] = page_token
        data = http_get_json(url, params)
        if not data:
            break
        for it in data.get("items", []):
            sn = it.get("snippet", {})
            rid = sn.get("resourceId", {})
            vid = rid.get("videoId")
            title = sn.get("title", "")
            thumbs = sn.get("thumbnails", {})
            # æŒ‰è´¨é‡ä¼˜å…ˆ
            thumb_url = (
                (thumbs.get("maxres") or {}).get("url") or
                (thumbs.get("standard") or {}).get("url") or
                (thumbs.get("high") or {}).get("url") or
                (thumbs.get("medium") or {}).get("url") or
                (thumbs.get("default") or {}).get("url") or
                ""
            )
            if vid:
                results.append({
                    "id": vid,
                    "title": title,
                    "thumbnail": thumb_url,
                    "url": f"https://www.youtube.com/watch?v={vid}",
                })
                if len(results) >= max_count:
                    break
        if len(results) >= max_count:
            break
        page_token = data.get("nextPageToken")
        if not page_token:
            break
    return results


def save_data_file(name: str, channel_id: str, channel_title: str, videos: List[dict]) -> Path:
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    path = DATA_DIR / f"{name}.json"
    payload = {
        "channel_id": channel_id,
        "channel_name": channel_title or name,
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "videos": videos,
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    return path


def pick_best_thumbnail(snippet: dict) -> Optional[str]:
    thumbs = (snippet or {}).get("thumbnails", {})
    for key in ["maxres", "standard", "high", "medium", "default"]:
        url = (thumbs.get(key) or {}).get("url")
        if url:
            return url
    return None


def download_channel_avatar(channel_id: str, api_key: str, save_name: str) -> Tuple[Optional[Path], Optional[Path]]:
    """ä¸‹è½½é¢‘é“å¤´åƒå¹¶ç”Ÿæˆç¼©ç•¥å›¾ï¼Œè¿”å› (åŸå›¾è·¯å¾„, ç¼©ç•¥å›¾è·¯å¾„)ã€‚"""
    # æ‹‰å–é¢‘é“ä¿¡æ¯ä»¥è·å–ç¼©ç•¥å›¾
    ch = http_get_json(
        "https://www.googleapis.com/youtube/v3/channels",
        {"part": "snippet", "id": channel_id, "key": api_key}
    )
    try:
        snippet = ch.get("items", [])[0]["snippet"]
    except Exception:
        snippet = None
    img_url = pick_best_thumbnail(snippet) if snippet else None
    if not img_url:
        return None, None

    # ä¸‹è½½å›¾ç‰‡
    try:
        req = urllib.request.Request(img_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=20) as resp:
            content = resp.read()
    except Exception:
        return None, None

    IMG_DIR.mkdir(parents=True, exist_ok=True)
    IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
    raw_path = IMG_DIR / f"{save_name}.jpg"
    resized_path = IMG_RESIZED_DIR / f"{save_name}.jpg"

    # ä¿å­˜åŸå›¾
    try:
        with open(raw_path, "wb") as f:
            f.write(content)
    except Exception:
        raw_path = None

    # ç”Ÿæˆç¼©ç•¥å›¾
    try:
        if Image is None:
            return raw_path, None
        with Image.open(BytesIO(content)) as im:
            # è½¬æˆRGBï¼Œç­‰æ¯”ç¼©æ”¾åˆ° 128x128 ç”»å¸ƒå†…ï¼Œå†å±…ä¸­é“ºæ»¡è£å‰ªï¼ˆæ–¹å½¢ï¼‰
            im = im.convert("RGB")
            size = 128
            # å…ˆæŒ‰çŸ­è¾¹ç­‰æ¯”æ”¾å¤§ï¼Œåå±…ä¸­è£å‰ª
            ratio = max(size / im.width, size / im.height)
            new_w, new_h = int(im.width * ratio), int(im.height * ratio)
            im = im.resize((new_w, new_h), Image.LANCZOS)
            left = (new_w - size) // 2
            top = (new_h - size) // 2
            im = im.crop((left, top, left + size, top + size))
            im.save(resized_path, format="JPEG", quality=88, optimize=True)
    except Exception:
        resized_path = None

    return raw_path, resized_path


def main():
    parser = argparse.ArgumentParser(description="ä»…æ·»åŠ é¢‘é“åˆ° japan_tv_youtube_channels.jsonï¼ˆä¸æŠ“å–ï¼‰")
    parser.add_argument("--url", help="YouTubeé¢‘é“URLï¼ˆå¦‚ https://www.youtube.com/@handleï¼‰")
    parser.add_argument("--name", help="æ˜¾ç¤ºåç§°ï¼ˆé»˜è®¤ç”¨ handle/IDï¼‰")
    parser.add_argument("--category", default="ãã®ä»–", help="åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ï¼‰")
    parser.add_argument("--subcategory", default="ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«", help="å­åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰")
    args = parser.parse_args()

    url = args.url
    if not url:
        try:
            url = input("è¯·è¾“å…¥YouTubeé¢‘é“åœ°å€: ").strip()
        except EOFError:
            print("âŒ æœªæä¾›URL")
            sys.exit(1)

    url = normalize_url(url)
    if not url:
        print("âŒ URL ä¸ºç©º")
        sys.exit(1)

    # é»˜è®¤åç§°ï¼šä»URLæå– handle/IDï¼Œå¹¶å¯¹ç™¾åˆ†å·ç¼–ç è¿›è¡Œè§£ç 
    name = (args.name or urllib.parse.unquote(extract_handle_or_id(url))).strip()
    if not name:
        print("âŒ æ— æ³•ç¡®å®šé¢‘é“åç§°ï¼Œè¯·ä½¿ç”¨ --name æŒ‡å®š")
        sys.exit(1)

    print("=== é¢„è§ˆ ===")
    print(f"URL: {url}")
    print(f"åç§°: {name}")
    print(f"åˆ†ç±»/å­åˆ†ç±»: {args.category} / {args.subcategory}")

    ok = upsert_channel(url=url, name=name, category=args.category, subcategory=args.subcategory)
    if not ok:
        sys.exit(1)

    # æŠ“å–è¯¥URLå¯¹åº”é¢‘é“å¹¶ç”Ÿæˆ data/{åç§°}.json
    print("\n=== æ­£åœ¨è¯»å– API Key å¹¶æŠ“å–è¯¥é¢‘é“ ===")
    keys = load_api_keys()
    if not keys:
        print("âŒ æœªåœ¨ WEB-INF/config.properties ä¸­æ‰¾åˆ° youtube.apikeyï¼Œæ— æ³•æŠ“å–ã€‚ä»…å®Œæˆæ·»åŠ åˆ°é…ç½®ã€‚")
        print("è¯·é…ç½® API Key åå†è¿è¡Œæœ¬è„šæœ¬ã€‚")
        sys.exit(0)

    # è½®æ¢ API Key è§£æé¢‘é“ID
    ch_id, ch_title = None, None
    api_key = None
    for k in keys:
        api_key = k
        ch_id, ch_title = resolve_channel_id(url, k)
        if ch_id:
            break
    if not ch_id:
        print("âŒ æ— æ³•è§£æé¢‘é“IDï¼ŒæŠ“å–ç»ˆæ­¢ã€‚å·²å®Œæˆæ·»åŠ åˆ°é…ç½®ã€‚")
        sys.exit(0)

    videos = fetch_channel_uploads(ch_id, api_key, max_count=200)
    out_path = save_data_file(name=name, channel_id=ch_id, channel_title=ch_title or name, videos=videos)

    # ä¸‹è½½å¹¶ç”Ÿæˆå¤´åƒç¼©ç•¥å›¾ï¼ˆä¸ä¸­æ–­ä¸»æµç¨‹ï¼‰
    raw_img, resized_img = download_channel_avatar(channel_id=ch_id, api_key=api_key, save_name=name)

    print(f"\nâœ… æŠ“å–å®Œæˆï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
    if raw_img:
        print(f"âœ… å·²ä¸‹è½½å¤´åƒï¼š{raw_img}")
    if resized_img:
        print(f"âœ… å·²ç”Ÿæˆç¼©ç•¥å›¾ï¼š{resized_img}")
    else:
        print("âš ï¸ æœªç”Ÿæˆç¼©ç•¥å›¾ï¼ˆå¯èƒ½æœªå®‰è£… Pillow æˆ–ä¸‹è½½å¤±è´¥ï¼‰")
    print("ğŸ‘‰ è¯·åˆ·æ–°ç½‘é¡µæŸ¥çœ‹è¯¥é¢‘é“è§†é¢‘ä¸å¤´åƒã€‚")


if __name__ == "__main__":
    main()


