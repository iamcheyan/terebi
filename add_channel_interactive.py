#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTubeé¢‘é“æ·»åŠ å·¥å…· - æ”¯æŒæ‰¹é‡æ·»åŠ 
ç”¨æ³•ï¼š
- äº¤äº’æ¨¡å¼ï¼špython add_channel_interactive.py
- å•ä¸ªé¢‘é“ï¼špython add_channel_interactive.py --url @handle
- æ‰¹é‡æ·»åŠ ï¼špython add_channel_interactive.py --url @handle1 @handle2 @handle3
"""

import argparse
import json
import os
import sys
import urllib.parse
import urllib.request
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Tuple

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent
CONFIG_FILE = PROJECT_ROOT / "all_channels.json"
DATA_DIR = PROJECT_ROOT / "data"
IMG_DIR = PROJECT_ROOT / "img"
IMG_RESIZED_DIR = IMG_DIR / "resized"

# å°è¯•å¯¼å…¥ Pillow
try:
    from PIL import Image
    from io import BytesIO
except ImportError:
    Image = None
    BytesIO = None


def load_config() -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not CONFIG_FILE.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_FILE}")
        sys.exit(1)
    
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_config(config: dict) -> bool:
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    try:
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
        return False


def load_api_keys() -> List[str]:
    """ä» WEB-INF/config.properties è¯»å– YouTube API Key"""
    config_file = PROJECT_ROOT / "WEB-INF" / "config.properties"
    if not config_file.exists():
        return []
    
    keys = []
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("youtube.apikey") and "=" in line:
                    key = line.split("=", 1)[1].strip()
                    if key:
                        keys.append(key)
    except Exception:
        pass
    
    return keys


def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ–URLæ ¼å¼"""
    if not url:
        return ""
    
    # å¦‚æœåªæ˜¯handleï¼ˆå¦‚ @handleï¼‰ï¼Œè¡¥å……å®Œæ•´URL
    if url.startswith("@"):
        return f"https://www.youtube.com/{url}"
    
    # ç¡®ä¿æ˜¯å®Œæ•´çš„YouTube URL
    if not url.startswith("http"):
        if url.startswith("www.youtube.com"):
            url = "https://" + url
        elif url.startswith("youtube.com"):
            url = "https://www." + url
        else:
            url = "https://www.youtube.com/" + url
    
    return url


def extract_handle_or_id(url: str) -> str:
    """ä»URLæå–handleæˆ–ID"""
    if not url:
        return ""
    
    # å¤„ç† @handle æ ¼å¼
    if "/@" in url:
        return url.split("/@")[-1].split("?")[0].split("/")[0]
    
    # å¤„ç† /channel/UC... æ ¼å¼
    if "/channel/" in url:
        return url.split("/channel/")[-1].split("?")[0].split("/")[0]
    
    # å¤„ç† /c/channel_name æ ¼å¼
    if "/c/" in url:
        return url.split("/c/")[-1].split("?")[0].split("/")[0]
    
    return ""


def check_channel_exists(url: str) -> bool:
    """æ£€æŸ¥é¢‘é“æ˜¯å¦å·²å­˜åœ¨"""
    config = load_config()
    
    for cat_data in config.values():
        if not isinstance(cat_data, dict):
            continue
        for subcat_data in cat_data.values():
            if not isinstance(subcat_data, list):
                continue
            for channel in subcat_data:
                if isinstance(channel, dict) and channel.get("url") == url:
                    return True
    return False


def generate_bakname(url: str, name: str) -> str:
    """ç”Ÿæˆé¢‘é“çš„bakname"""
    # ä¼˜å…ˆä»URLæå–handle
    handle_or_id = extract_handle_or_id(url)
    if handle_or_id and not handle_or_id.startswith("UC"):
        # å¦‚æœæ˜¯handleï¼ˆä¸æ˜¯channel IDï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        return handle_or_id
    
    # å¦‚æœæ˜¯channel IDï¼Œå°è¯•ä»URLæå–å…¶ä»–æ ‡è¯†ç¬¦
    if "/@" in url:
        handle = url.split("/@")[-1].split("?")[0].split("/")[0]
        if handle:
            return handle
    
    # å¦‚æœURLä¸­æœ‰/c/è·¯å¾„ï¼Œæå–é¢‘é“å
    if "/c/" in url:
        channel_name = url.split("/c/")[-1].split("?")[0].split("/")[0]
        if channel_name:
            return channel_name
    
    # æœ€åä½¿ç”¨æ¸…ç†åçš„é¢‘é“åç§°
    import re
    safe_name = re.sub(r'[^\w\-]', '_', name)
    safe_name = re.sub(r'_+', '_', safe_name).strip('_')
    if not safe_name:
        safe_name = "channel"
    return safe_name


def upsert_channel(url: str, name: str, category: str, subcategory: str) -> bool:
    """æ·»åŠ æˆ–æ›´æ–°é¢‘é“åˆ°é…ç½®"""
    config = load_config()
    
    # ç”Ÿæˆbakname
    bakname = generate_bakname(url, name)
    
    # æŸ¥æ‰¾ç°æœ‰é¢‘é“
    for cat_data in config.values():
        if not isinstance(cat_data, dict):
            continue
        for subcat_data in cat_data.values():
            if not isinstance(subcat_data, list):
                continue
            for i, channel in enumerate(subcat_data):
                if isinstance(channel, dict) and channel.get("url") == url:
                    # æ›´æ–°ç°æœ‰é¢‘é“
                    subcat_data[i] = {
                        "name": name,
                        "url": url,
                        "bakname": bakname,
                        "cached": False,
                        "skip": False
                    }
                    print(f"âœ… å·²æ›´æ–°é¢‘é“: {name} (bakname: {bakname})")
                    return save_config(config)
    
    # æ·»åŠ æ–°é¢‘é“
    if category not in config:
        config[category] = {}
    if subcategory not in config[category]:
        config[category][subcategory] = []
    
    new_channel = {
        "name": name,
        "url": url,
        "bakname": bakname,
        "cached": False,
        "skip": False
    }
    
    config[category][subcategory].append(new_channel)
    print(f"âœ… å·²æ·»åŠ é¢‘é“: {name} (bakname: {bakname})")
    return save_config(config)


def http_get_json(url: str, params: dict) -> dict:
    """å‘é€HTTP GETè¯·æ±‚å¹¶è¿”å›JSON"""
    if params:
        query = urllib.parse.urlencode(params)
        url = f"{url}?{query}"
    
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            # æ£€æŸ¥APIå“åº”ä¸­çš„é”™è¯¯
            if "error" in data:
                error_info = data["error"]
                print(f"âš ï¸ APIé”™è¯¯: {error_info.get('message', 'Unknown error')}")
                if "quotaExceeded" in str(error_info):
                    print("ğŸ’¡ æç¤º: APIé…é¢å·²ç”¨å®Œï¼Œè¯·ç­‰å¾…æˆ–ä½¿ç”¨å…¶ä»–API Key")
                elif "forbidden" in str(error_info).lower():
                    print("ğŸ’¡ æç¤º: API Keyå¯èƒ½æ— æ•ˆæˆ–æƒé™ä¸è¶³")
            return data
    except Exception as e:
        print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {e}")
        return {}


def resolve_channel_id(url: str, api_key: str) -> Tuple[Optional[str], Optional[str]]:
    """è§£æé¢‘é“IDå’Œæ ‡é¢˜"""
    handle_or_id = extract_handle_or_id(url)
    if not handle_or_id:
        return None, None
    
    # å°è¯•ä½œä¸ºhandleæŸ¥è¯¢
    if not handle_or_id.startswith("UC"):
        try:
            # è§£ç ç™¾åˆ†å·ç¼–ç 
            decoded_handle = urllib.parse.unquote(handle_or_id)
            resp = http_get_json(
                "https://www.googleapis.com/youtube/v3/channels",
                {
                    "part": "snippet",
                    "forUsername": decoded_handle,
                    "key": api_key
                }
            )
            items = resp.get("items", [])
            if items:
                return items[0]["id"], items[0]["snippet"]["title"]
        except Exception:
            pass
    
    # å°è¯•ä½œä¸ºIDæŸ¥è¯¢
    try:
        resp = http_get_json(
            "https://www.googleapis.com/youtube/v3/channels",
            {
                "part": "snippet",
                "id": handle_or_id,
                "key": api_key
            }
        )
        items = resp.get("items", [])
        if items:
            return items[0]["id"], items[0]["snippet"]["title"]
    except Exception:
        pass
    
    return None, None


def resolve_channel_id_via_html(url: str) -> Tuple[Optional[str], Optional[str]]:
    """åœ¨æ— APIæƒ…å†µä¸‹ï¼Œé€šè¿‡æŠ“å–é¢‘é“é¡µHTMLæå– channelId å’Œæ ‡é¢˜ã€‚
    é€‚é… @handle ä¸ /channel/UC... é“¾æ¥ã€‚
    """
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode("utf-8", errors="ignore")
    except Exception as e:
        print(f"âŒ HTMLæŠ“å–å¤±è´¥: {e}")
        return None, None

    # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾channelId
    patterns = [
        r'"channelId"\s*:\s*"(UC[\w-]{22})"',
        r'"externalId"\s*:\s*"(UC[\w-]{22})"',
        r'"ucid"\s*:\s*"(UC[\w-]{22})"',
        r'"channelId":"(UC[\w-]{22})"',
        r'channelId.*?"(UC[\w-]{22})"',
    ]
    
    channel_id = None
    for pattern in patterns:
        m = re.search(pattern, html)
        if m:
            channel_id = m.group(1)
            break

    # æŠ“å–æ ‡é¢˜ï¼ˆä¼˜å…ˆ og:titleï¼Œå…¶æ¬¡ <title>ï¼‰
    title = None
    m_title = re.search(r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\']([^"\']+)["\']', html)
    if m_title:
        title = m_title.group(1).strip()
    else:
        m_title2 = re.search(r"<title>(.*?)</title>", html, re.S)
        if m_title2:
            title = re.sub(r"\s+\-\s+YouTube$", "", m_title2.group(1).strip())

    return channel_id, title


def fetch_channel_uploads_via_rss(channel_id: str, max_count: int = 200) -> List[dict]:
    """æ— éœ€APIï¼Œä½¿ç”¨YouTubeå®˜æ–¹RSSè·å–è§†é¢‘åˆ—è¡¨ã€‚
    å‚è€ƒ: https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}
    """
    if not channel_id:
        return []
    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        req = urllib.request.Request(feed_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            xml_text = resp.read()
    except Exception as e:
        print(f"âŒ RSSè·å–å¤±è´¥: {e}")
        return []

    try:
        root = ET.fromstring(xml_text)
    except Exception as e:
        print(f"âŒ RSSè§£æå¤±è´¥: {e}")
        return []

    ns = {
        'atom': 'http://www.w3.org/2005/Atom',
        'media': 'http://search.yahoo.com/mrss/'
    }
    entries = root.findall('atom:entry', ns)

    videos: List[dict] = []
    for entry in entries[:max_count]:
        # å®‰å…¨åœ°è·å–video_id
        video_id = None
        try:
            video_id_el = entry.find('yt:videoId', {'yt': 'http://www.youtube.com/xml/schemas/2015'})
            if video_id_el is not None and hasattr(video_id_el, 'text'):
                video_id = video_id_el.text
        except Exception:
            pass
        
        if not video_id:
            # å¤‡ç”¨ï¼šä» link href ä¸­è§£æ v å‚æ•°
            try:
                link_el = entry.find('atom:link', ns)
                if link_el is not None:
                    href = link_el.get('href', '')
                    q = urllib.parse.urlparse(href).query
                    qs = urllib.parse.parse_qs(q)
                    video_id = (qs.get('v') or [''])[0]
            except Exception:
                pass

        # å®‰å…¨åœ°è·å–æ ‡é¢˜
        title = ""
        try:
            title_el = entry.find('atom:title', ns)
            if title_el is not None and hasattr(title_el, 'text'):
                title = title_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–å‘å¸ƒæ—¶é—´
        published_at = ""
        try:
            published_el = entry.find('atom:published', ns)
            if published_el is not None and hasattr(published_el, 'text'):
                published_at = published_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–ç¼©ç•¥å›¾
        thumb_url = None
        try:
            media_group = entry.find('media:group', ns)
            if media_group is not None:
                thumb = media_group.find('media:thumbnail', ns)
                if thumb is not None:
                    thumb_url = thumb.get('url')
        except Exception:
            pass

        videos.append({
            "id": video_id or "",
            "title": title,
            "description": "",
            "publishedAt": published_at,
            "thumbnails": {"default": {"url": thumb_url}} if thumb_url else {},
            "url": f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
        })

    return videos


def fetch_channel_uploads(channel_id: str, api_key: str, max_count: int = 200) -> List[dict]:
    """è·å–é¢‘é“ä¸Šä¼ çš„è§†é¢‘åˆ—è¡¨"""
    videos = []
    next_page_token = None
    
    while len(videos) < max_count:
        params = {
            "part": "snippet",
            "channelId": channel_id,
            "type": "video",
            "order": "date",
            "maxResults": min(50, max_count - len(videos)),
            "key": api_key
        }
        
        if next_page_token:
            params["pageToken"] = next_page_token
        
        resp = http_get_json("https://www.googleapis.com/youtube/v3/search", params)
        items = resp.get("items", [])
        
        if not items:
            break
        
        for item in items:
            snippet = item.get("snippet", {})
            video = {
                "id": item.get("id", {}).get("videoId", ""),
                "title": snippet.get("title", ""),
                "description": snippet.get("description", ""),
                "publishedAt": snippet.get("publishedAt", ""),
                "thumbnails": snippet.get("thumbnails", {}),
                "url": f"https://www.youtube.com/watch?v={item.get('id', {}).get('videoId', '')}"
            }
            videos.append(video)
        
        next_page_token = resp.get("nextPageToken")
        if not next_page_token:
            break
    
    return videos[:max_count]


def save_data_file(name: str, channel_id: str, channel_title: str, videos: List[dict], bakname: str = None) -> Path:
    """ä¿å­˜è§†é¢‘æ•°æ®åˆ°JSONæ–‡ä»¶"""
    DATA_DIR.mkdir(exist_ok=True)
    
    payload = {
        "channel_id": channel_id,
        "channel_name": channel_title or name,
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "videos": videos,
    }
    
    # ä¼˜å…ˆä½¿ç”¨baknameï¼Œå¦åˆ™æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
    if bakname:
        safe_name = bakname
    else:
        safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_', '.')).rstrip()
        if not safe_name:
            safe_name = "channel"
    
    out_path = DATA_DIR / f"{safe_name}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    
    return out_path


def pick_best_thumbnail(snippet: dict) -> Optional[str]:
    """é€‰æ‹©æœ€ä½³ç¼©ç•¥å›¾URL"""
    thumbs = (snippet or {}).get("thumbnails", {})
    for key in ["maxres", "standard", "high", "medium", "default"]:
        url = (thumbs.get(key) or {}).get("url")
        if url:
            return url
    return None


def download_channel_avatar(channel_id: str, api_key: str, save_name: str) -> Tuple[Optional[Path], Optional[Path]]:
    """ä¸‹è½½é¢‘é“å¤´åƒå¹¶ç”Ÿæˆç¼©ç•¥å›¾ï¼Œè¿”å› (åŸå›¾è·¯å¾„, ç¼©ç•¥å›¾è·¯å¾„)"""
    # æ‹‰å–é¢‘é“ä¿¡æ¯ä»¥è·å–ç¼©ç•¥å›¾
    ch = http_get_json(
        "https://www.googleapis.com/youtube/v3/channels",
        {"part": "snippet", "id": channel_id, "key": api_key}
    )
    try:
        snippet = ch.get("items", [])[0]["snippet"]
    except Exception:
        snippet = None
    img_url = pick_best_thumbnail(snippet) if snippet else None
    if not img_url:
        return None, None

    # ä¸‹è½½å›¾ç‰‡
    try:
        req = urllib.request.Request(img_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=20) as resp:
            content = resp.read()
    except Exception:
        return None, None

    IMG_DIR.mkdir(parents=True, exist_ok=True)
    IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
    raw_path = IMG_DIR / f"{save_name}.jpg"
    resized_path = IMG_RESIZED_DIR / f"{save_name}.jpg"

    # ä¿å­˜åŸå›¾
    try:
        with open(raw_path, "wb") as f:
            f.write(content)
    except Exception:
        raw_path = None

    # ç”Ÿæˆç¼©ç•¥å›¾
    try:
        if Image is None:
            return raw_path, None
        with Image.open(BytesIO(content)) as im:
            # è½¬æˆRGBï¼Œç­‰æ¯”ç¼©æ”¾åˆ° 128x128 ç”»å¸ƒå†…ï¼Œå†å±…ä¸­é“ºæ»¡è£å‰ªï¼ˆæ–¹å½¢ï¼‰
            im = im.convert("RGB")
            size = 128
            # å…ˆæŒ‰çŸ­è¾¹ç­‰æ¯”æ”¾å¤§ï¼Œåå±…ä¸­è£å‰ª
            ratio = max(size / im.width, size / im.height)
            new_w, new_h = int(im.width * ratio), int(im.height * ratio)
            im = im.resize((new_w, new_h), Image.LANCZOS)
            left = (new_w - size) // 2
            top = (new_h - size) // 2
            im = im.crop((left, top, left + size, top + size))
            im.save(resized_path, format="JPEG", quality=88, optimize=True)
    except Exception:
        resized_path = None

    return raw_path, resized_path


def process_single_channel(url: str, name: str, category: str, subcategory: str, bakname: str = None) -> bool:
    """å¤„ç†å•ä¸ªé¢‘é“ï¼šæ·»åŠ åˆ°é…ç½®å¹¶æŠ“å–æ•°æ®"""
    url = normalize_url(url)
    if not url:
        print(f"âŒ URL æ— æ•ˆ: {url}")
        return False

    # é»˜è®¤åç§°ï¼šä»URLæå– handle/IDï¼Œå¹¶å¯¹ç™¾åˆ†å·ç¼–ç è¿›è¡Œè§£ç 
    if not name:
        name = urllib.parse.unquote(extract_handle_or_id(url)).strip()
        if not name:
            print(f"âŒ æ— æ³•ç¡®å®šé¢‘é“åç§°: {url}")
            return False

    print(f"=== å¤„ç†é¢‘é“: {name} ===")
    print(f"URL: {url}")
    print(f"åˆ†ç±»/å­åˆ†ç±»: {category} / {subcategory}")

    # ç”Ÿæˆbakname
    bakname = generate_bakname(url, name)
    
    # æ£€æŸ¥é¢‘é“æ˜¯å¦å·²å­˜åœ¨
    if check_channel_exists(url):
        print(f"â­ï¸ é¢‘é“å·²å­˜åœ¨ï¼Œä½†éœ€è¦æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        # ä½¿ç”¨ç”Ÿæˆçš„baknameæ£€æŸ¥æ•°æ®æ–‡ä»¶
        data_path = DATA_DIR / f"{bakname}.json"
        if data_path.exists():
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦åŒ…å«è§†é¢‘æ•°æ®
            try:
                with open(data_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    video_count = len(data.get("videos", []))
                    if video_count > 0:
                        print(f"âœ… æ•°æ®æ–‡ä»¶å·²å­˜åœ¨ï¼ˆ{video_count}æ¡è§†é¢‘ï¼‰ï¼Œå®Œå…¨è·³è¿‡: {name}")
                        return True
                    else:
                        print(f"âš ï¸ æ•°æ®æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼Œéœ€è¦é‡æ–°æŠ“å–: {name}")
            except Exception:
                print(f"âš ï¸ æ•°æ®æ–‡ä»¶æŸåï¼Œéœ€è¦é‡æ–°æŠ“å–: {name}")
        else:
            print(f"âŒ é¢‘é“å·²å­˜åœ¨ä½†ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹æŠ“å–æ•°æ®...")
            # ç»§ç»­æ‰§è¡ŒæŠ“å–æ“ä½œï¼Œä¸è¿”å›
    else:
        # æ·»åŠ åˆ°é…ç½®
        ok = upsert_channel(url=url, name=name, category=category, subcategory=subcategory)
        if not ok:
            print(f"âŒ æ·»åŠ å¤±è´¥: {name}")
            return False
        print(f"âœ… å·²æ·»åŠ åˆ°é…ç½®: {name}")

    # æŠ“å–è¯¥URLå¯¹åº”é¢‘é“å¹¶ç”Ÿæˆ data/{åç§°}.json
    print("=== æ­£åœ¨è¯»å– API Key å¹¶æŠ“å–è¯¥é¢‘é“ï¼ˆå¸¦HTML/RSSå›é€€ï¼‰ ===")
    keys = load_api_keys()
    # å…ˆå°è¯•æ— éœ€APIçš„HTMLè§£æè·å– channelId
    ch_id_html, ch_title_html = resolve_channel_id_via_html(url)
    if ch_id_html:
        # æ— APIï¼šç”¨RSSæŠ“å–è§†é¢‘
        videos = fetch_channel_uploads_via_rss(ch_id_html, max_count=200)
        out_path = save_data_file(name=name, channel_id=ch_id_html, channel_title=ch_title_html or name, videos=videos, bakname=bakname)
        # å¤´åƒï¼šå°è¯•ä»HTMLæå– og:image
        try:
            req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urllib.request.urlopen(req, timeout=20) as resp:
                html = resp.read().decode("utf-8", errors="ignore")
            m_img = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html)
            if m_img and Image is not None:
                # ä¸‹è½½å¹¶ç”Ÿæˆç¼©ç•¥å›¾
                try:
                    req2 = urllib.request.Request(m_img.group(1), headers={"User-Agent": "Mozilla/5.0"})
                    with urllib.request.urlopen(req2, timeout=20) as resp2:
                        content = resp2.read()
                    IMG_DIR.mkdir(parents=True, exist_ok=True)
                    IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
                    raw_path = IMG_DIR / f"{bakname}.jpg"
                    with open(raw_path, "wb") as f:
                        f.write(content)
                    with Image.open(BytesIO(content)) as im:
                        im = im.convert("RGB")
                        size = 128
                        ratio = max(size / im.width, size / im.height)
                        new_w, new_h = int(im.width * ratio), int(im.height * ratio)
                        im = im.resize((new_w, new_h), Image.LANCZOS)
                        left = (new_w - size) // 2
                        top = (new_h - size) // 2
                        im = im.crop((left, top, left + size, top + size))
                        im.save(IMG_RESIZED_DIR / f"{bakname}.jpg", format="JPEG", quality=88, optimize=True)
                except Exception:
                    pass
        except Exception:
            pass
        print(f"âœ… æŠ“å–å®Œæˆï¼ˆRSSï¼‰ï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
        return True

    if not keys:
        print("âŒ æœªæ‰¾åˆ° API Keyï¼Œä¸”HTMLè§£æå¤±è´¥ã€‚ä»…å®Œæˆæ·»åŠ åˆ°é…ç½®å¹¶ç”Ÿæˆç©ºæ•°æ®ã€‚")
        out_path = save_data_file(name=name, channel_id="", channel_title=name, videos=[], bakname=bakname)
        print(f"ğŸ§© å·²ç”Ÿæˆç©ºæ•°æ®æ–‡ä»¶ï¼š{out_path}")
        # å ä½å¤´åƒ
        try:
            placeholder = IMG_RESIZED_DIR / "placeholder.jpg"
            target_logo = IMG_RESIZED_DIR / f"{bakname}.jpg"
            IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
            if placeholder.exists() and not target_logo.exists():
                import shutil
                shutil.copyfile(placeholder, target_logo)
        except Exception:
            pass
        return True

    # è½®æ¢ API Key è§£æé¢‘é“ID
    ch_id, ch_title = None, None
    api_key = None
    for k in keys:
        api_key = k
        ch_id, ch_title = resolve_channel_id(url, k)
        if ch_id:
            break
    if not ch_id:
        # å†æ¬¡å°è¯•HTMLè§£æ + RSSï¼ˆAPIå¤±è´¥å¯èƒ½403/é…é¢/é™åˆ¶ï¼‰
        ch_id_html, ch_title_html = resolve_channel_id_via_html(url)
        if ch_id_html:
            videos = fetch_channel_uploads_via_rss(ch_id_html, max_count=200)
            out_path = save_data_file(name=name, channel_id=ch_id_html, channel_title=ch_title_html or name, videos=videos, bakname=bakname)
            print(f"âœ… æŠ“å–å®Œæˆï¼ˆRSSï¼‰ï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
            return True
        print("âŒ æ— æ³•è§£æé¢‘é“IDï¼ŒæŠ“å–ç»ˆæ­¢ã€‚å·²å®Œæˆæ·»åŠ åˆ°é…ç½®ã€‚")
        # ä»ç„¶ç”Ÿæˆç©ºçš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…å‰ç«¯404
        out_path = save_data_file(name=name, channel_id="", channel_title=name, videos=[], bakname=bakname)
        print(f"ğŸ§© å·²ç”Ÿæˆç©ºæ•°æ®æ–‡ä»¶ï¼š{out_path}")
        # å ä½å¤´åƒ
        try:
            placeholder = IMG_RESIZED_DIR / "placeholder.jpg"
            target_logo = IMG_RESIZED_DIR / f"{bakname}.jpg"
            IMG_RESIZED_DIR.mkdir(parents=True, exist_ok=True)
            if placeholder.exists() and not target_logo.exists():
                import shutil
                shutil.copyfile(placeholder, target_logo)
        except Exception:
            pass
        return True

    videos = fetch_channel_uploads(ch_id, api_key, max_count=200)
    out_path = save_data_file(name=name, channel_id=ch_id, channel_title=ch_title or name, videos=videos, bakname=bakname)

    # ä¸‹è½½å¹¶ç”Ÿæˆå¤´åƒç¼©ç•¥å›¾ï¼ˆä¸ä¸­æ–­ä¸»æµç¨‹ï¼‰
    raw_img, resized_img = download_channel_avatar(channel_id=ch_id, api_key=api_key, save_name=bakname)

    print(f"âœ… æŠ“å–å®Œæˆï¼š{len(videos)} æ¡è§†é¢‘ â†’ {out_path}")
    if raw_img:
        print(f"âœ… å·²ä¸‹è½½å¤´åƒï¼š{raw_img}")
    if resized_img:
        print(f"âœ… å·²ç”Ÿæˆç¼©ç•¥å›¾ï¼š{resized_img}")
    else:
        print("âš ï¸ æœªç”Ÿæˆç¼©ç•¥å›¾ï¼ˆå¯èƒ½æœªå®‰è£… Pillow æˆ–ä¸‹è½½å¤±è´¥ï¼‰")
    
    return True


def load_youtube_channels() -> dict:
    """åŠ è½½youtube_channels.jsonæ–‡ä»¶"""
    channels_file = DATA_DIR / "youtube_channels.json"
    if not channels_file.exists():
        print(f"âŒ youtube_channels.json æ–‡ä»¶ä¸å­˜åœ¨: {channels_file}")
        return {}
    
    try:
        with open(channels_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å– youtube_channels.json å¤±è´¥: {e}")
        return {}


def test_api_keys() -> bool:
    """æµ‹è¯•API Keyæ˜¯å¦å¯ç”¨"""
    keys = load_api_keys()
    if not keys:
        print("âŒ æœªæ‰¾åˆ°API Key")
        return False
    
    print("=== æµ‹è¯•API Key ===")
    for i, key in enumerate(keys, 1):
        print(f"æµ‹è¯•API Key {i}...")
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„APIè°ƒç”¨æ¥æµ‹è¯•
        resp = http_get_json(
            "https://www.googleapis.com/youtube/v3/channels",
            {"part": "snippet", "id": "UC_x5XG1OV2P6uZZ5FSM9Ttw", "key": key}
        )
        if resp and "items" in resp:
            print(f"âœ… API Key {i} å¯ç”¨")
            return True
        else:
            print(f"âŒ API Key {i} ä¸å¯ç”¨")
    
    print("âŒ æ‰€æœ‰API Keyéƒ½ä¸å¯ç”¨")
    return False


def process_json_channels() -> None:
    """å¤„ç†youtube_channels.jsonä¸­çš„é¢‘é“ï¼Œæ£€æŸ¥æ•°æ®æ–‡ä»¶å¹¶åˆå§‹åŒ–ç¼ºå¤±çš„é¢‘é“"""
    channels_data = load_youtube_channels()
    if not channels_data:
        return
    
    print("=== æ£€æµ‹ youtube_channels.json ä¸­çš„é¢‘é“ ===")
    
    # å…ˆæµ‹è¯•API Key
    if not test_api_keys():
        print("âš ï¸ è­¦å‘Š: æ‰€æœ‰API Keyéƒ½ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨RSSæ–¹å¼æŠ“å–ï¼ˆå¯èƒ½æ•°æ®ä¸å®Œæ•´ï¼‰")
    
    total_channels = 0
    missing_channels = 0
    processed_channels = 0
    
    for category, channels in channels_data.items():
        if not isinstance(channels, list):
            continue
            
        print(f"\n--- æ£€æŸ¥åˆ†ç±»: {category} ---")
        
        for channel in channels:
            if not isinstance(channel, dict):
                continue
                
            total_channels += 1
            name = channel.get("name", "")
            url = channel.get("url", "")
            
            if not name or not url:
                continue
                
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä¼˜å…ˆä½¿ç”¨baknameï¼Œå¦åˆ™ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åï¼‰
            bakname = channel.get("bakname", "")
            if bakname:
                safe_name = bakname
            else:
                safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_', '.')).rstrip()
                if not safe_name:
                    safe_name = "channel"
            data_file = DATA_DIR / f"{safe_name}.json"
            
            if data_file.exists():
                # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦åŒ…å«è§†é¢‘æ•°æ®
                try:
                    with open(data_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        video_count = len(data.get("videos", []))
                        if video_count > 0:
                            print(f"âœ… {name} - æ•°æ®æ–‡ä»¶å·²å­˜åœ¨ï¼ˆ{video_count}æ¡è§†é¢‘ï¼‰ï¼Œè·³è¿‡")
                            continue
                        else:
                            print(f"âš ï¸ {name} - æ•°æ®æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼Œéœ€è¦é‡æ–°æŠ“å–")
                except Exception:
                    print(f"âš ï¸ {name} - æ•°æ®æ–‡ä»¶æŸåï¼Œéœ€è¦é‡æ–°æŠ“å–")
            
            print(f"âŒ {name} - ç¼ºå°‘æ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            missing_channels += 1
            
            # æå–åˆ†ç±»ä¿¡æ¯
            category_name = category
            subcategory_name = "ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«"  # é»˜è®¤å­åˆ†ç±»
            
            # å¤„ç†é¢‘é“
            try:
                if process_single_channel(url, name, category_name, subcategory_name, bakname):
                    processed_channels += 1
                    print(f"âœ… {name} - åˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ {name} - åˆå§‹åŒ–å¤±è´¥")
            except Exception as e:
                print(f"âŒ {name} - åˆå§‹åŒ–å¤±è´¥: {e}")
    
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"æ€»é¢‘é“æ•°: {total_channels}")
    print(f"ç¼ºå°‘æ•°æ®æ–‡ä»¶: {missing_channels}")
    print(f"æˆåŠŸåˆå§‹åŒ–: {processed_channels}")
    if processed_channels > 0:
        print("ğŸ‘‰ è¯·åˆ·æ–°ç½‘é¡µæŸ¥çœ‹æ–°é¢‘é“")


def main():
    parser = argparse.ArgumentParser(description="æ·»åŠ YouTubeé¢‘é“åˆ°é…ç½®å¹¶æŠ“å–æ•°æ®")
    parser.add_argument("--url", nargs="+", help="YouTubeé¢‘é“URLï¼ˆæ”¯æŒå¤šä¸ªï¼Œå¦‚ --url @handle1 @handle2ï¼‰")
    parser.add_argument("--name", help="æ˜¾ç¤ºåç§°ï¼ˆé»˜è®¤ç”¨ handle/IDï¼‰")
    parser.add_argument("--category", default="ãã®ä»–", help="åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ï¼‰")
    parser.add_argument("--subcategory", default="ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«", help="å­åˆ†ç±»ï¼ˆé»˜è®¤: ãã®ä»–ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰")
    parser.add_argument("--json", action="store_true", help="æ£€æµ‹youtube_channels.jsonä¸­çš„é¢‘é“å¹¶åˆå§‹åŒ–ç¼ºå¤±çš„æ•°æ®æ–‡ä»¶")
    parser.add_argument("--yes", "-y", action="store_true", help="è‡ªåŠ¨ç¡®è®¤ï¼Œä¸è¯¢é—®")
    args = parser.parse_args()

    if args.json:
        # JSONæ¨¡å¼ï¼šæ£€æµ‹youtube_channels.jsonä¸­çš„é¢‘é“
        process_json_channels()
        return

    if args.url:
        # æ‰¹é‡æ·»åŠ æ¨¡å¼
        urls = args.url
        category = args.category
        subcategory = args.subcategory
        
        print(f"=== æ‰¹é‡æ·»åŠ æ¨¡å¼ ===")
        print(f"é¢‘é“æ•°é‡: {len(urls)}")
        print(f"åˆ†ç±»/å­åˆ†ç±»: {category} / {subcategory}")
        print(f"URLåˆ—è¡¨:")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        if not args.yes:
            try:
                if input(f"\nç¡®è®¤æ‰¹é‡æ·»åŠ  {len(urls)} ä¸ªé¢‘é“ï¼Ÿ(y/N): ").lower() != 'y':
                    print("å·²å–æ¶ˆ")
                    return
            except EOFError:
                print("âŒ æ— æ³•è¯»å–è¾“å…¥ï¼Œè¯·ä½¿ç”¨ --yes å‚æ•°è‡ªåŠ¨ç¡®è®¤")
                return
        
        success_count = 0
        for i, url in enumerate(urls, 1):
            print(f"\n=== å¤„ç†ç¬¬ {i}/{len(urls)} ä¸ªé¢‘é“ ===")
            try:
                if process_single_channel(url, args.name, category, subcategory):
                    success_count += 1
                    print(f"âœ… ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å®Œæˆ")
                else:
                    print(f"âŒ ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å¤±è´¥")
            except Exception as e:
                print(f"âŒ ç¬¬ {i} ä¸ªé¢‘é“å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"\n=== æ‰¹é‡å¤„ç†å®Œæˆ ===")
        print(f"æˆåŠŸ: {success_count}/{len(urls)} ä¸ªé¢‘é“")
        if success_count > 0:
            print("ğŸ‘‰ è¯·åˆ·æ–°ç½‘é¡µæŸ¥çœ‹æ–°é¢‘é“")
    else:
        # äº¤äº’æ¨¡å¼
        try:
            url = input("è¯·è¾“å…¥YouTubeé¢‘é“åœ°å€: ").strip()
        except EOFError:
            print("âŒ æœªæä¾›URL")
            sys.exit(1)

        if not url:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„URL")
            return

        name = args.name
        if not name:
            name = urllib.parse.unquote(extract_handle_or_id(url)).strip()
            if not name:
                print("âŒ æ— æ³•ç¡®å®šé¢‘é“åç§°ï¼Œè¯·ä½¿ç”¨ --name æŒ‡å®š")
                sys.exit(1)

        print("=== é¢„è§ˆ ===")
        print(f"URL: {url}")
        print(f"åç§°: {name}")
        print(f"åˆ†ç±»/å­åˆ†ç±»: {args.category} / {args.subcategory}")

        if input("\nç¡®è®¤æ·»åŠ ï¼Ÿ(y/N): ").lower() != 'y':
            print("å·²å–æ¶ˆ")
            return

        process_single_channel(url, name, args.category, args.subcategory)


if __name__ == "__main__":
    main()