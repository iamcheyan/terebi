#!/usr/bin/env python3
"""
åˆå¹¶é‡å¤çš„æ•°æ®æ–‡ä»¶
å°†æ—§æ ¼å¼çš„æ–‡ä»¶åˆå¹¶åˆ°æ–°æ ¼å¼çš„baknameæ–‡ä»¶ä¸­
"""
import json
from pathlib import Path
from datetime import datetime, timezone

DATA_DIR = Path("data")

def load_json_file(file_path: Path):
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None

def save_json_file(file_path: Path, data):
    """ä¿å­˜JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return False

def merge_videos(old_videos, new_videos):
    """åˆå¹¶è§†é¢‘æ•°æ®ï¼Œå»é‡å¹¶ä¿æŒæœ€æ–°æ•°æ®åœ¨å‰"""
    # åˆ›å»ºè§†é¢‘IDåˆ°è§†é¢‘æ•°æ®çš„æ˜ å°„
    video_map = {}
    
    # å…ˆæ·»åŠ æ–°æ•°æ®ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
    for video in new_videos:
        video_id = video.get('video_id') or video.get('id')
        if video_id:
            video_map[video_id] = video
    
    # å†æ·»åŠ æ—§æ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨çš„è¯ï¼‰
    for video in old_videos:
        video_id = video.get('video_id') or video.get('id')
        if video_id and video_id not in video_map:
            video_map[video_id] = video
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    merged_videos = list(video_map.values())
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    def get_published_time(video):
        published = video.get('published', '')
        if published:
            try:
                # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
                if published.endswith('Z'):
                    published = published.replace('Z', '+00:00')
                elif '+' not in published and '-' not in published[-6:]:
                    # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºUTC
                    published = published + '+00:00'
                return datetime.fromisoformat(published)
            except:
                pass
        return datetime.min.replace(tzinfo=timezone.utc)
    
    merged_videos.sort(key=get_published_time, reverse=True)
    
    return merged_videos

def find_duplicate_files():
    """æŸ¥æ‰¾é‡å¤çš„æ–‡ä»¶"""
    duplicates = []
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = list(DATA_DIR.glob("*.json"))
    
    for file_path in json_files:
        if file_path.name == "youtube_channels.json":
            continue
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„channel_UCæ–‡ä»¶
        if file_path.name.startswith("channel_UC"):
            continue
            
        # å°è¯•æ‰¾åˆ°å¯¹åº”çš„channel_UCæ–‡ä»¶
        data = load_json_file(file_path)
        if not data:
            continue
            
        channel_name = data.get('channel_name', '')
        if not channel_name:
            continue
            
        # æŸ¥æ‰¾å¯èƒ½çš„channel_UCæ–‡ä»¶
        for uc_file in DATA_DIR.glob("channel_UC*.json"):
            uc_data = load_json_file(uc_file)
            if uc_data and uc_data.get('channel_name') == channel_name:
                duplicates.append((file_path, uc_file, channel_name))
                break
    
    return duplicates

def merge_duplicate_files():
    """åˆå¹¶é‡å¤æ–‡ä»¶"""
    print("=== æŸ¥æ‰¾é‡å¤æ–‡ä»¶ ===")
    duplicates = find_duplicate_files()
    
    if not duplicates:
        print("âœ… æ²¡æœ‰æ‰¾åˆ°é‡å¤æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(duplicates)} å¯¹é‡å¤æ–‡ä»¶")
    
    merged_count = 0
    
    for old_file, new_file, channel_name in duplicates:
        print(f"\n=== å¤„ç†: {channel_name} ===")
        print(f"æ—§æ–‡ä»¶: {old_file.name}")
        print(f"æ–°æ–‡ä»¶: {new_file.name}")
        
        # åŠ è½½æ•°æ®
        old_data = load_json_file(old_file)
        new_data = load_json_file(new_file)
        
        if not old_data or not new_data:
            print(f"âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè·³è¿‡")
            continue
        
        # åˆå¹¶è§†é¢‘æ•°æ®
        old_videos = old_data.get('videos', [])
        new_videos = new_data.get('videos', [])
        
        print(f"æ—§æ–‡ä»¶è§†é¢‘æ•°: {len(old_videos)}")
        print(f"æ–°æ–‡ä»¶è§†é¢‘æ•°: {len(new_videos)}")
        
        merged_videos = merge_videos(old_videos, new_videos)
        print(f"åˆå¹¶åè§†é¢‘æ•°: {len(merged_videos)}")
        
        # åˆ›å»ºåˆå¹¶åçš„æ•°æ®
        merged_data = {
            "channel_name": channel_name,
            "updated_at": datetime.now(timezone.utc).isoformat(),
            "videos": merged_videos
        }
        
        # ä¿å­˜åˆ°æ–°æ–‡ä»¶
        if save_json_file(new_file, merged_data):
            print(f"âœ… å·²åˆå¹¶åˆ°: {new_file.name}")
            
            # åˆ é™¤æ—§æ–‡ä»¶
            try:
                old_file.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {old_file.name}")
                merged_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥: {e}")
        else:
            print(f"âŒ åˆå¹¶å¤±è´¥")
    
    print(f"\n=== åˆå¹¶å®Œæˆ ===")
    print(f"æˆåŠŸåˆå¹¶: {merged_count} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    merge_duplicate_files()
