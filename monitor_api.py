#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIé…é¢ç›‘æ§è„šæœ¬
ç”¨äºç›‘æ§YouTube APIçš„ä½¿ç”¨æƒ…å†µå’Œé¢‘é“æ›´æ–°çŠ¶æ€
"""

import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
import requests
import configparser

PROJECT_ROOT = Path(__file__).parent
DATA_DIR = PROJECT_ROOT / "data"
LOG_DIR = PROJECT_ROOT / "log"

def load_api_keys():
    """åŠ è½½APIå¯†é’¥"""
    config_file = PROJECT_ROOT / "WEB-INF" / "config.properties"
    if not config_file.exists():
        return []
    
    config = configparser.ConfigParser()
    config.read(config_file)
    api_keys = []
    
    for key, value in config['DEFAULT'].items():
        if key.startswith('youtube.apikey'):
            api_keys.append(value)
    
    return api_keys

def check_channel_data_status():
    """æ£€æŸ¥é¢‘é“æ•°æ®çŠ¶æ€"""
    print("=== é¢‘é“æ•°æ®çŠ¶æ€æ£€æŸ¥ ===")
    
    if not DATA_DIR.exists():
        print("âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return
    
    json_files = list(DATA_DIR.glob("*.json"))
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    # ç»Ÿè®¡è§†é¢‘æ•°é‡
    total_videos = 0
    empty_channels = 0
    recent_updates = 0
    
    cutoff_time = datetime.now() - timedelta(days=1)
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            videos = data.get('videos', [])
            video_count = len(videos)
            total_videos += video_count
            
            if video_count == 0:
                empty_channels += 1
            
            # æ£€æŸ¥æ›´æ–°æ—¶é—´
            updated_at = data.get('updated_at', '')
            if updated_at:
                try:
                    update_time = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                    if update_time > cutoff_time:
                        recent_updates += 1
                except:
                    pass
                    
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {json_file.name} - {e}")
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»è§†é¢‘æ•°: {total_videos}")
    print(f"  - ç©ºé¢‘é“æ•°: {empty_channels}")
    print(f"  - æœ€è¿‘æ›´æ–°: {recent_updates} ä¸ªé¢‘é“")
    print(f"  - æ•°æ®å®Œæ•´æ€§: {((len(json_files) - empty_channels) / len(json_files) * 100):.1f}%")

def estimate_api_usage():
    """ä¼°ç®—APIä½¿ç”¨æƒ…å†µ"""
    print("\n=== APIä½¿ç”¨æƒ…å†µä¼°ç®— ===")
    
    api_keys = load_api_keys()
    if not api_keys:
        print("âŒ æœªæ‰¾åˆ°APIå¯†é’¥")
        return
    
    print(f"ğŸ”‘ å¯ç”¨APIå¯†é’¥: {len(api_keys)} ä¸ª")
    
    # è®¡ç®—ç†è®ºé…é¢
    total_quota = len(api_keys) * 10000  # æ¯ä¸ªå¯†é’¥æ¯å¤©10,000å•ä½
    safe_quota = total_quota * 0.5  # ä½¿ç”¨50%çš„å®‰å…¨é…é¢
    
    print(f"ğŸ“ˆ é…é¢ä¿¡æ¯:")
    print(f"  - æ€»é…é¢: {total_quota:,} å•ä½/å¤©")
    print(f"  - å®‰å…¨é…é¢: {safe_quota:,} å•ä½/å¤©")
    
    # ä¼°ç®—å½“å‰é…ç½®çš„APIä½¿ç”¨
    videos_per_channel = 200
    api_calls_per_channel = (videos_per_channel + 49) // 50  # å‘ä¸Šå–æ•´
    quota_per_channel = api_calls_per_channel * 100  # æ¯æ¬¡æœç´¢è°ƒç”¨æ¶ˆè€—100å•ä½
    
    print(f"ğŸ”¢ å½“å‰é…ç½®ä¼°ç®—:")
    print(f"  - æ¯é¢‘é“è§†é¢‘æ•°: {videos_per_channel}")
    print(f"  - æ¯é¢‘é“APIè°ƒç”¨: {api_calls_per_channel} æ¬¡")
    print(f"  - æ¯é¢‘é“é…é¢æ¶ˆè€—: {quota_per_channel} å•ä½")
    
    # ä¼°ç®—å¯å¤„ç†çš„é¢‘é“æ•°
    max_channels = safe_quota // quota_per_channel
    print(f"  - å¯å¤„ç†é¢‘é“æ•°: çº¦ {max_channels} ä¸ªé¢‘é“")

def check_recent_logs():
    """æ£€æŸ¥æœ€è¿‘çš„æ—¥å¿—"""
    print("\n=== æœ€è¿‘æ—¥å¿—æ£€æŸ¥ ===")
    
    if not LOG_DIR.exists():
        print("âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æŸ¥æ‰¾æœ€è¿‘çš„æ—¥å¿—æ–‡ä»¶
    log_files = list(LOG_DIR.glob("cron_update_*.log"))
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°æ›´æ–°æ—¥å¿—")
        return
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ—¥å¿—
    latest_log = max(log_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“„ æœ€æ–°æ—¥å¿—: {latest_log.name}")
    
    try:
        with open(latest_log, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†ææ—¥å¿—å†…å®¹
        lines = content.split('\n')
        error_lines = [line for line in lines if 'é”™è¯¯' in line or 'å¤±è´¥' in line or 'ERROR' in line.upper()]
        success_lines = [line for line in lines if 'æˆåŠŸ' in line or 'å®Œæˆ' in line or 'SUCCESS' in line.upper()]
        
        print(f"ğŸ“Š æ—¥å¿—åˆ†æ:")
        print(f"  - æ€»è¡Œæ•°: {len(lines)}")
        print(f"  - é”™è¯¯è¡Œæ•°: {len(error_lines)}")
        print(f"  - æˆåŠŸè¡Œæ•°: {len(success_lines)}")
        
        if error_lines:
            print(f"âš ï¸ æœ€è¿‘é”™è¯¯:")
            for error in error_lines[-3:]:  # æ˜¾ç¤ºæœ€å3ä¸ªé”™è¯¯
                print(f"    {error}")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {e}")

def main():
    print("ğŸ” Terebi APIç›‘æ§å·¥å…·")
    print("=" * 50)
    
    check_channel_data_status()
    estimate_api_usage()
    check_recent_logs()
    
    print("\n" + "=" * 50)
    print("âœ… ç›‘æ§å®Œæˆ")

if __name__ == "__main__":
    main()
