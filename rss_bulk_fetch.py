#!/usr/bin/env python3
"""
ä½¿ç”¨RSSæ‰¹é‡æŠ“å–æ‰€æœ‰é¢‘é“æ•°æ®çš„ç¨‹åº
éå†youtube_channels.jsonå’Œjapan_tv_youtube_channels.jsonä¸­çš„æ‰€æœ‰é¢‘é“
ä½¿ç”¨RSSæ–¹å¼æŠ“å–æ•°æ®ï¼Œæœ‰æ–°çš„è§†é¢‘å°±æ·»åŠ 
"""
import json
import re
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Optional

# é…ç½®
DATA_DIR = Path("data")
CHANNELS_FILE = DATA_DIR / "youtube_channels.json"
JAPAN_CHANNELS_FILE = Path("all_channels.json")

def load_channels_from_file(file_path: Path) -> Dict:
    """åŠ è½½é¢‘é“é…ç½®æ–‡ä»¶"""
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return {}

def extract_channel_id_from_url(url: str) -> Optional[str]:
    """ä»YouTube URLä¸­æå–é¢‘é“ID"""
    if not url:
        return None
    
    # å¤„ç†ä¸åŒçš„URLæ ¼å¼
    patterns = [
        r'@([\w-]+)',  # @usernameæ ¼å¼
        r'/channel/(UC[\w-]{22})',  # /channel/UC...æ ¼å¼
        r'/c/([\w-]+)',  # /c/usernameæ ¼å¼
        r'/user/([\w-]+)',  # /user/usernameæ ¼å¼
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    return None

def get_rss_url(channel_id: str) -> str:
    """æ ¹æ®é¢‘é“IDç”ŸæˆRSS URL"""
    if channel_id.startswith('UC'):
        # å®Œæ•´çš„é¢‘é“ID
        return f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    else:
        # ç”¨æˆ·åï¼Œéœ€è¦å…ˆè·å–é¢‘é“ID
        return f"https://www.youtube.com/@{channel_id}"

def fetch_channel_id_via_html(username: str) -> Optional[str]:
    """é€šè¿‡HTMLé¡µé¢è·å–é¢‘é“ID"""
    try:
        url = f"https://www.youtube.com/@{username}"
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        with urllib.request.urlopen(req, timeout=10) as response:
            html = response.read().decode('utf-8')
            
            # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°é¢‘é“ID
            patterns = [
                r'"channelId"\s*:\s*"(UC[\w-]{22})"',
                r'"externalId"\s*:\s*"(UC[\w-]{22})"',
                r'"ucid"\s*:\s*"(UC[\w-]{22})"',
                r'"channelId":"(UC[\w-]{22})"',
                r'channelId.*?"(UC[\w-]{22})"',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, html)
                if match:
                    return match.group(1)
            
            return None
            
    except Exception as e:
        print(f"âŒ HTMLæŠ“å–å¤±è´¥: {e}")
        return None

def fetch_videos_via_rss(rss_url: str) -> List[Dict]:
    """é€šè¿‡RSSæŠ“å–è§†é¢‘æ•°æ®"""
    try:
        req = urllib.request.Request(rss_url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8')
            
            # è§£æXML
            root = ET.fromstring(content)
            
            videos = []
            for item in root.findall('.//{http://www.w3.org/2005/Atom}entry'):
                try:
                    # æå–è§†é¢‘ä¿¡æ¯
                    video_id_elem = item.find('.//{http://www.youtube.com/xml/schemas/2015}videoId')
                    title_elem = item.find('.//{http://www.w3.org/2005/Atom}title')
                    link_elem = item.find('.//{http://www.w3.org/2005/Atom}link')
                    published_elem = item.find('.//{http://www.w3.org/2005/Atom}published')
                    
                    if video_id_elem is not None and title_elem is not None:
                        video_id = video_id_elem.text
                        title = title_elem.text
                        link = link_elem.get('href') if link_elem is not None else f"https://www.youtube.com/watch?v={video_id}"
                        published = published_elem.text if published_elem is not None else ""
                        
                        # æå–ç¼©ç•¥å›¾URL
                        thumbnail_url = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
                        
                        video_data = {
                            "video_id": video_id,
                            "title": title,
                            "url": link,
                            "published": published,
                            "thumbnail_url": thumbnail_url
                        }
                        
                        videos.append(video_data)
                        
                except Exception as e:
                    print(f"âš ï¸ è§£æè§†é¢‘æ¡ç›®å¤±è´¥: {e}")
                    continue
            
            return videos
            
    except Exception as e:
        print(f"âŒ RSSæŠ“å–å¤±è´¥: {e}")
        return []

def load_existing_data(bakname: str) -> Dict:
    """åŠ è½½ç°æœ‰æ•°æ®æ–‡ä»¶"""
    data_file = DATA_DIR / f"{bakname}.json"
    if not data_file.exists():
        return {"videos": []}
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
        return {"videos": []}

def save_data_file(bakname: str, channel_name: str, videos: List[Dict]) -> Path:
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    DATA_DIR.mkdir(exist_ok=True)
    
    payload = {
        "channel_name": channel_name,
        "updated_at": datetime.now(timezone.utc).isoformat(),
        "videos": videos,
    }
    
    out_path = DATA_DIR / f"{bakname}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    
    return out_path

def process_channel(channel: Dict, category: str) -> bool:
    """å¤„ç†å•ä¸ªé¢‘é“"""
    name = channel.get("name", "")
    url = channel.get("url", "")
    bakname = channel.get("bakname", "")
    
    if not name or not url:
        print(f"âš ï¸ è·³è¿‡æ— æ•ˆé¢‘é“: {name}")
        return False
    
    if not bakname:
        print(f"âš ï¸ è·³è¿‡æ— baknameçš„é¢‘é“: {name}")
        return False
    
    print(f"=== å¤„ç†é¢‘é“: {name} ===")
    print(f"URL: {url}")
    print(f"åˆ†ç±»: {category}")
    print(f"Bakname: {bakname}")
    
    # æå–é¢‘é“ID
    channel_id = extract_channel_id_from_url(url)
    if not channel_id:
        print(f"âŒ æ— æ³•æå–é¢‘é“ID: {name}")
        return False
    
    # å¦‚æœæ˜¯ç”¨æˆ·åï¼Œå°è¯•è·å–å®Œæ•´é¢‘é“ID
    if not channel_id.startswith('UC'):
        print(f"ğŸ” å°è¯•è·å–å®Œæ•´é¢‘é“ID: {channel_id}")
        full_channel_id = fetch_channel_id_via_html(channel_id)
        if full_channel_id:
            channel_id = full_channel_id
            print(f"âœ… è·å–åˆ°å®Œæ•´é¢‘é“ID: {channel_id}")
        else:
            print(f"âš ï¸ æ— æ³•è·å–å®Œæ•´é¢‘é“IDï¼Œä½¿ç”¨ç”¨æˆ·å: {channel_id}")
    
    # ç”ŸæˆRSS URL
    if channel_id.startswith('UC'):
        rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    else:
        rss_url = f"https://www.youtube.com/feeds/videos.xml?user={channel_id}"
    
    print(f"RSS URL: {rss_url}")
    
    # æŠ“å–æ–°æ•°æ®
    print("ğŸ“¥ æŠ“å–RSSæ•°æ®...")
    new_videos = fetch_videos_via_rss(rss_url)
    
    if not new_videos:
        print(f"âŒ æœªè·å–åˆ°è§†é¢‘æ•°æ®: {name}")
        return False
    
    print(f"âœ… è·å–åˆ° {len(new_videos)} æ¡è§†é¢‘")
    
    # åŠ è½½ç°æœ‰æ•°æ®
    existing_data = load_existing_data(bakname)
    existing_videos = existing_data.get("videos", [])
    existing_video_ids = {video.get("video_id") for video in existing_videos}
    
    # è¿‡æ»¤æ–°è§†é¢‘
    new_video_ids = {video.get("video_id") for video in new_videos}
    truly_new_videos = [video for video in new_videos if video.get("video_id") not in existing_video_ids]
    
    if truly_new_videos:
        print(f"ğŸ†• å‘ç° {len(truly_new_videos)} æ¡æ–°è§†é¢‘")
        # åˆå¹¶æ•°æ®ï¼ˆæ–°è§†é¢‘åœ¨å‰ï¼‰
        all_videos = truly_new_videos + existing_videos
    else:
        print(f"â„¹ï¸ æ²¡æœ‰æ–°è§†é¢‘ï¼Œæ›´æ–°ç°æœ‰æ•°æ®")
        all_videos = new_videos  # ä½¿ç”¨æœ€æ–°çš„RSSæ•°æ®
    
    # ä¿å­˜æ•°æ®
    out_path = save_data_file(bakname, name, all_videos)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {out_path}")
    
    return True

def collect_all_channels() -> List[tuple]:
    """æ”¶é›†æ‰€æœ‰é¢‘é“ä¿¡æ¯"""
    all_channels = []
    
    # åŠ è½½youtube_channels.json
    youtube_data = load_channels_from_file(CHANNELS_FILE)
    for category, channels in youtube_data.items():
        if isinstance(channels, list):
            for channel in channels:
                if isinstance(channel, dict) and channel.get("bakname"):
                    all_channels.append((channel, category))
    
    # åŠ è½½japan_tv_youtube_channels.json
    japan_data = load_channels_from_file(JAPAN_CHANNELS_FILE)
    for category, channels in japan_data.items():
        if isinstance(channels, list):
            for channel in channels:
                if isinstance(channel, dict) and channel.get("bakname"):
                    all_channels.append((channel, category))
        elif isinstance(channels, dict):
            # å¤„ç†åµŒå¥—ç»“æ„
            for sub_category, sub_channels in channels.items():
                if isinstance(sub_channels, list):
                    for channel in sub_channels:
                        if isinstance(channel, dict) and channel.get("bakname"):
                            all_channels.append((channel, f"{category}/{sub_category}"))
    
    return all_channels

def main():
    """ä¸»å‡½æ•°"""
    print("=== RSSæ‰¹é‡æŠ“å–æ‰€æœ‰é¢‘é“æ•°æ® ===")
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    
    # æ”¶é›†æ‰€æœ‰é¢‘é“
    all_channels = collect_all_channels()
    print(f"æ‰¾åˆ° {len(all_channels)} ä¸ªé¢‘é“")
    
    if not all_channels:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢‘é“")
        return
    
    # å¤„ç†æ¯ä¸ªé¢‘é“
    success_count = 0
    error_count = 0
    
    for i, (channel, category) in enumerate(all_channels, 1):
        print(f"\n--- å¤„ç†ç¬¬ {i}/{len(all_channels)} ä¸ªé¢‘é“ ---")
        
        try:
            if process_channel(channel, category):
                success_count += 1
                print(f"âœ… æˆåŠŸå¤„ç†: {channel.get('name', 'Unknown')}")
            else:
                error_count += 1
                print(f"âŒ å¤„ç†å¤±è´¥: {channel.get('name', 'Unknown')}")
        except Exception as e:
            error_count += 1
            print(f"âŒ å¤„ç†å¼‚å¸¸: {channel.get('name', 'Unknown')} - {e}")
    
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"æ€»é¢‘é“æ•°: {len(all_channels)}")
    print(f"æˆåŠŸå¤„ç†: {success_count}")
    print(f"å¤„ç†å¤±è´¥: {error_count}")
    print("ğŸ‘‰ è¯·åˆ·æ–°ç½‘é¡µæŸ¥çœ‹æ›´æ–°")

if __name__ == "__main__":
    main()
