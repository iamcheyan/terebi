import json
import re
import random
import os
import urllib.parse
import urllib.request
import urllib.error
from datetime import datetime
import subprocess
import traceback
import argparse
import time
import sys

# ç»Ÿä¸€è·¯å¾„ï¼Œæ”¯æŒä»é¡¹ç›®æ ¹æˆ– tools ç›®å½•æ‰§è¡Œ
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
LOG_DIR = os.path.join(PROJECT_ROOT, 'log')
# æ—¶é—´æˆ³æ—¥å¿—æ–‡ä»¶ï¼Œé¿å…è¦†ç›–
_LOG_TS = datetime.now().strftime('%Y%m%d_%H%M%S')
LOG_FILE = os.path.join(LOG_DIR, f'get_channel_videos_{_LOG_TS}.log')

# å°†æ ‡å‡†è¾“å‡º/é”™è¯¯åŒæ—¶å†™å…¥æ–‡ä»¶ä¸æ§åˆ¶å°
class _Tee:
    def __init__(self, stream, file_path):
        self._stream = stream
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
        except Exception:
            pass
        # æŒ‰è¿½åŠ æ–¹å¼å†™å…¥ï¼Œä¿è¯utf-8
        self._file = open(file_path, 'a', encoding='utf-8', buffering=1)

    def write(self, data):
        try:
            self._stream.write(data)
        except Exception:
            pass
        try:
            # ä¸ºæ¯ä¸€è¡Œæ·»åŠ æ—¶é—´å‰ç¼€ï¼Œæå‡å¯è¯»æ€§
            if data:
                lines = data.splitlines(True)
                for line in lines:
                    if line.endswith('\n'):
                        self._file.write(f"[{datetime.now().isoformat()}] {line}")
                    else:
                        self._file.write(f"[{datetime.now().isoformat()}] {line}\n")
        except Exception:
            pass

    def flush(self):
        try:
            self._stream.flush()
        except Exception:
            pass
        try:
            self._file.flush()
        except Exception:
            pass

    def close(self):
        try:
            self._file.close()
        except Exception:
            pass

# å®‰è£… Teeï¼Œä»…åœ¨ä½œä¸ºè„šæœ¬è¿è¡Œæ—¶ç”Ÿæ•ˆ
if __name__ == '__main__':
    try:
        sys.stdout = _Tee(sys.stdout, LOG_FILE)
        sys.stderr = _Tee(sys.stderr, LOG_FILE)
    except Exception:
        pass

"""
# åŸºæœ¬ä½¿ç”¨ï¼Œæ¯ä¸ªé¢‘é“è·å–500ä¸ªè§†é¢‘
python get_channel_videos.py


# æŒ‡å®šæ¯ä¸ªé¢‘é“è·å–1000ä¸ªè§†é¢‘
python get_channel_videos.py --videos-per-channel 1000


# è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ï¼Œè‡ªåŠ¨ç®¡ç†APIé…é¢
åŠŸèƒ½ï¼šæŒ‡å®šæ¯ä¸ªé¢‘é“è·å–çš„è§†é¢‘æ•°é‡ä¸º250ä¸ª
é€‚ä¸­æ•°é‡ï¼š250ä¸ªè§†é¢‘æ˜¯ä¸€ä¸ªå¹³è¡¡çš„é€‰æ‹©ï¼Œæ—¢èƒ½è·å–è¶³å¤Ÿçš„å†…å®¹ï¼Œåˆä¸ä¼šè¿‡åº¦æ¶ˆè€—APIé…é¢
APIè°ƒç”¨ï¼šæ¯ä¸ªé¢‘é“çº¦éœ€5æ¬¡APIè°ƒç”¨ï¼ˆ250Ã·50=5ï¼‰
é…é¢è®¡ç®—ï¼šç³»ç»Ÿä¼šæ ¹æ®è¿™ä¸ªæ•°å€¼è®¡ç®—æ¯ä¸ªAPIå¯†é’¥å¯ä»¥å¤„ç†çš„é¢‘é“æ•°é‡

python get_channel_videos.py --auto-task --videos-per-channel 250 --yes


# å¼ºåˆ¶æ›´æ–°æ‰€æœ‰é¢‘é“ï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
python get_channel_videos.py --force
"""

# æœ€å¤§ç»“æœæ•°
MAX_RESULTS = 50

# è¯»å–JSONæ–‡ä»¶ï¼ˆé¡¹ç›®æ ¹ï¼‰
with open(os.path.join(PROJECT_ROOT, 'all_channels.json'), 'r', encoding='utf-8') as file:
    data = json.load(file)

# åˆå§‹åŒ–URLåˆ—è¡¨
channel_search_urls = []

# ä»é…ç½®æ–‡ä»¶è¯»å–APIå¯†é’¥
import configparser
config = configparser.ConfigParser()
config.read(os.path.join(PROJECT_ROOT, 'WEB-INF', 'config.properties'))
api_keys = []

# è¯»å–å¹¶æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„APIå¯†é’¥
print("\n=== å¯ç”¨çš„APIå¯†é’¥ ===")
for key, value in config['DEFAULT'].items():
    if key.startswith('youtube.apikey'):
        print(f"å¯†é’¥ {key}: {value[:15]}...{value[-5:]}")  # åªæ˜¾ç¤ºå¯†é’¥çš„ä¸€éƒ¨åˆ†ï¼Œä¿æŠ¤å®‰å…¨
        api_keys.append(value)

if not api_keys:
    raise ValueError("æœªèƒ½ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–åˆ°ä»»ä½•APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„å’Œå†…å®¹æ˜¯å¦æ­£ç¡®")

# éšæœºé€‰æ‹©ä¸€ä¸ªAPIå¯†é’¥å¹¶æ˜¾ç¤º
API_KEY = random.choice(api_keys)
print(f"\nå½“å‰é€‰æ‹©ä½¿ç”¨çš„APIå¯†é’¥: {API_KEY[:15]}...{API_KEY[-5:]}")

# åˆå§‹åŒ–URLå’Œåç§°çš„å­—å…¸
channel_search_info = []

# å¤„ç†æ‰€æœ‰åˆ†ç±»çš„é¢‘é“
def process_channels(channels_data, is_nested=True):
    """
    å¤„ç†é¢‘é“æ•°æ®ï¼Œæå–URLå¹¶æ„å»ºAPIæœç´¢URL
    
    å‚æ•°:
    channels_data - é¢‘é“æ•°æ®ï¼Œå¯ä»¥æ˜¯åˆ—è¡¨æˆ–å­—å…¸
    is_nested - æ˜¯å¦æ˜¯åµŒå¥—ç»“æ„ï¼ˆå¦‚åœ°æ–¹æ”¾é€å±€ï¼‰
    """
    result = []
    
    # ç»Ÿä¸€å¤„ç†æ‰€æœ‰å±‚çº§çš„é¢‘é“æ•°æ®
    channels_to_process = []
    
    # å°†ä¸åŒç»“æ„çš„æ•°æ®ç»Ÿä¸€è½¬æ¢ä¸ºé¢‘é“åˆ—è¡¨
    if isinstance(channels_data, dict):
        for category, items in channels_data.items():
            if isinstance(items, list):
                channels_to_process.extend(items)
    elif isinstance(channels_data, list):
        channels_to_process.extend(channels_data)
    
    # ç»Ÿä¸€å¤„ç†æ‰€æœ‰é¢‘é“
    for channel in channels_to_process:
        # å¦‚æœskipä¸ºtrueåˆ™è·³è¿‡å¤„ç†
        if channel.get("skip"):
            continue
            
        if channel.get("url"):
            # ä¼˜å…ˆä½¿ç”¨baknameä½œä¸ºæœç´¢å…³é”®è¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»URLæå–
            if channel.get("bakname") and channel.get("bakname").strip():
                keyword = channel.get("bakname").strip()
            else:
                # æå–URLä¸­çš„å…³é”®å­—ä½œä¸ºå¤‡é€‰
                match = re.search(r'(?:youtube\.com/(?:@|c/|channel/|user/)?)([^/]+)(?:/.*)?$', channel["url"])
                if match:
                    keyword = match.group(1)
                else:
                    keyword = channel["name"]  # æœ€åä½¿ç”¨é¢‘é“åç§°
            
            # æ„å»ºAPI URL
            api_url = f"https://www.googleapis.com/youtube/v3/search?part=snippet&q={urllib.parse.quote(keyword)}&type=channel&key={API_KEY}&maxResults=10"
            result.append({
                "name": channel["name"],
                "bakname": channel.get("bakname", ""),
                "url": api_url
            })
    
    return result

# å¤„ç†æ‰€æœ‰åˆ†ç±»
for category, channels in data.items():
    channel_search_info.extend(process_channels(channels))

# æ‰“å°ç»“æœæ•°é‡
print(f"æ€»å…±ç”Ÿæˆäº† {len(channel_search_info)} ä¸ªæœç´¢URL")
# for info in channel_search_info:
#     print(f"é¢‘é“åç§°: {info['name']}")
#     print(f"æœç´¢URL: {info['url']}")
#     print("-" * 50)

# é…ç½®requestsä¼šè¯ï¼Œæ·»åŠ é‡è¯•åŠŸèƒ½
def get_requests_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=5,  # æ€»å…±å°è¯•æ¬¡æ•°(åŒ…æ‹¬é¦–æ¬¡è¯·æ±‚)
        backoff_factor=1,  # é‡è¯•é—´éš” = {backoff factor} * (2 ** ({é‡è¯•æ¬¡æ•°} - 1))
        status_forcelist=[429, 500, 502, 503, 504],  # é‡åˆ°è¿™äº›çŠ¶æ€ç æ—¶é‡è¯•
        allowed_methods=["GET", "POST"]  # å…è®¸é‡è¯•çš„HTTPæ–¹æ³•
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

# ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚å‡½æ•°
def make_api_request(url, params=None, max_attempts=3, initial_delay=2):
    """
    å‘é€å¸¦æœ‰é‡è¯•æœºåˆ¶çš„APIè¯·æ±‚
    
    å‚æ•°:
    url - è¯·æ±‚URL
    params - è¯·æ±‚å‚æ•°
    max_attempts - æœ€å¤§å°è¯•æ¬¡æ•°
    initial_delay - åˆå§‹å»¶è¿Ÿæ—¶é—´(ç§’)
    
    è¿”å›:
    è¯·æ±‚å“åº”æˆ–None
    """
    session = get_requests_session()
    attempt = 0
    last_exception = None
    
    while attempt < max_attempts:
        try:
            response = session.get(url, params=params, timeout=(10, 30))  # è¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶30ç§’
            return response
        except (requests.ConnectionError, requests.Timeout) as e:
            attempt += 1
            last_exception = e
            delay = initial_delay * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿
            print(f"è¿æ¥å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯• ({attempt}/{max_attempts})...")
            time.sleep(delay)
    
    print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚å¤±è´¥: {str(last_exception)}")
    return None

# ä¿®æ”¹å„å¤„APIè¯·æ±‚ä»£ç 
def get_channel_info(channel_id):
    global API_KEY
    
    url = f'https://www.googleapis.com/youtube/v3/channels'
    params = {
        'part': 'snippet',
        'id': channel_id,
        'key': API_KEY
    }
    
    response = make_api_request(url, params)
    # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢APIå¯†é’¥
    if response is None or response.status_code != 200:
        API_KEY = try_switch_api_key(API_KEY)
        params['key'] = API_KEY
        response = make_api_request(url, params)
    
    if response is not None and response.status_code == 200:
        data = response.json()
        if data['items']:
            return data['items'][0]['snippet']['title']
    return channel_id  # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨channel_idä½œä¸ºåå¤‡åç§°

def get_channel_videos(channel_id):
    # è·å–ä¸Šä¼ æ’­æ”¾åˆ—è¡¨ID
    playlist_id = f'UU{channel_id[2:]}'
    
    url = f'https://www.googleapis.com/youtube/v3/playlistItems'
    
    params = {
        'part': 'snippet',
        'playlistId': playlist_id,
        'maxResults': MAX_RESULTS,
        'key': API_KEY
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        return response.json()['items']
    return []

def save_videos_to_json(channel_id, original_name):
    # ç¡®ä¿sourceç›®å½•å­˜åœ¨ï¼ˆé¡¹ç›®æ ¹ï¼‰
    source_dir = os.path.join(PROJECT_ROOT, 'source')
    if not os.path.exists(source_dir):
        os.makedirs(source_dir)
    
    # è·å–é¢‘é“åç§°
    channel_name = get_channel_info(channel_id)
    
    # è·å–è§†é¢‘åˆ—è¡¨
    videos = get_channel_videos(channel_id)
    
    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
    data = {
        'channel_id': channel_id,
        'channel_name': channel_name,
        'original_name': original_name,
        'updated_at': datetime.now().isoformat(),
        'videos': videos
    }
    
    # ä½¿ç”¨åŸå§‹åç§°ä½œä¸ºæ–‡ä»¶å
    # ä½¿ç”¨baknameä½œä¸ºæ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰baknameåˆ™ä½¿ç”¨é¢‘é“åç§°
    safe_name = info.get("bakname", "").strip()
    if not safe_name:
        safe_name = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in original_name)
    filename = os.path.join(source_dir, f'{safe_name}.json')
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename

# ä¿®æ­£ï¼šä½¿ç”¨channel_search_infoè€Œä¸æ˜¯channel_search_urls
shuffled_info = channel_search_info.copy()
random.shuffle(shuffled_info)

# å°†é¢‘é“åˆ†æˆ5ç»„ï¼Œæ¯ç»„ä½¿ç”¨ä¸åŒçš„APIå¯†é’¥
def process_channels_in_groups():
    # å‡è®¾æ‚¨æœ‰100ä¸ªé¢‘é“
    total_channels = len(shuffled_info)
    channels_per_group = (total_channels + len(api_keys) - 1) // len(api_keys)
    
    print(f"æ€»å…±æœ‰ {total_channels} ä¸ªé¢‘é“ï¼Œåˆ†æˆ {len(api_keys)} ç»„ï¼Œæ¯ç»„çº¦ {channels_per_group} ä¸ªé¢‘é“")
    
    for i, api_key in enumerate(api_keys):
        start_idx = i * channels_per_group
        end_idx = min((i + 1) * channels_per_group, total_channels)
        
        if start_idx >= total_channels:
            break
            
        group_channels = shuffled_info[start_idx:end_idx]
        print(f"\n=== å¤„ç†ç¬¬ {i+1} ç»„é¢‘é“ (ä½¿ç”¨APIå¯†é’¥ {api_key[:5]}...{api_key[-3:]}) ===")
        print(f"æœ¬ç»„åŒ…å« {len(group_channels)} ä¸ªé¢‘é“")
        
        # ä¸ºè¿™ç»„é¢‘é“è®¾ç½®å…¨å±€APIå¯†é’¥
        global API_KEY
        API_KEY = api_key
        
        # å¤„ç†è¿™ç»„é¢‘é“
        for info in group_channels:
            try:
                process_channel(info)
            except Exception as e:
                print(f"å¤„ç†é¢‘é“ {info['name']} æ—¶å‡ºé”™: {str(e)}")
                continue

# å¤„ç†å•ä¸ªé¢‘é“çš„å‡½æ•°
def fetch_channel_videos_via_rss(channel_id, max_count=200):
    """ä½¿ç”¨RSSæ–¹å¼è·å–é¢‘é“è§†é¢‘åˆ—è¡¨"""
    if not channel_id:
        return []
    
    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        import urllib.request
        import xml.etree.ElementTree as ET
        
        req = urllib.request.Request(feed_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            xml_text = resp.read()
    except Exception as e:
        print(f"âŒ RSSè·å–å¤±è´¥: {e}")
        return []

    try:
        root = ET.fromstring(xml_text)
    except Exception as e:
        print(f"âŒ RSSè§£æå¤±è´¥: {e}")
        return []

    ns = {
        'atom': 'http://www.w3.org/2005/Atom',
        'media': 'http://search.yahoo.com/mrss/'
    }
    entries = root.findall('atom:entry', ns)

    videos = []
    for entry in entries[:max_count]:
        # å®‰å…¨åœ°è·å–video_id
        video_id = None
        try:
            video_id_el = entry.find('yt:videoId', {'yt': 'http://www.youtube.com/xml/schemas/2015'})
            if video_id_el is not None and hasattr(video_id_el, 'text'):
                video_id = video_id_el.text
        except Exception:
            pass
        
        if not video_id:
            # å¤‡ç”¨ï¼šä» link href ä¸­è§£æ v å‚æ•°
            try:
                link_el = entry.find('atom:link', ns)
                if link_el is not None:
                    href = link_el.get('href', '')
                    q = urllib.parse.urlparse(href).query
                    qs = urllib.parse.parse_qs(q)
                    video_id = (qs.get('v') or [''])[0]
            except Exception:
                pass

        # å®‰å…¨åœ°è·å–æ ‡é¢˜
        title = ""
        try:
            title_el = entry.find('atom:title', ns)
            if title_el is not None and hasattr(title_el, 'text'):
                title = title_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–å‘å¸ƒæ—¶é—´
        published_at = ""
        try:
            published_el = entry.find('atom:published', ns)
            if published_el is not None and hasattr(published_el, 'text'):
                published_at = published_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–ç¼©ç•¥å›¾
        thumb_url = None
        try:
            media_group = entry.find('media:group', ns)
            if media_group is not None:
                thumb = media_group.find('media:thumbnail', ns)
                if thumb is not None:
                    thumb_url = thumb.get('url')
        except Exception:
            pass

        videos.append({
            "id": video_id or "",
            "title": title,
            "description": "",
            "publishedAt": published_at,
            "thumbnails": {"default": {"url": thumb_url}} if thumb_url else {},
            "url": f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
        })

    return videos


def process_channel(info, videos_per_channel=500, auto_confirm=False):
    global API_KEY
    
    print(f'\nå‡†å¤‡å¤„ç†é¢‘é“: {info["name"]}')
    
    # æ£€æŸ¥ç¼“å­˜
    # ä½¿ç”¨baknameä½œä¸ºæ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰baknameåˆ™ä½¿ç”¨é¢‘é“åç§°
    safe_name = info.get("bakname", "").strip()
    if not safe_name:
        safe_name = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in info["name"])
    data_filename = os.path.join(PROJECT_ROOT, 'data', f'{safe_name}.json')
    
    if os.path.exists(data_filename):
        print(f'å‘ç°ç°æœ‰ç¼“å­˜æ–‡ä»¶ï¼Œå‡†å¤‡æ£€æŸ¥æ˜¯å¦æœ‰æ–°è§†é¢‘: {data_filename}')
        # æ£€æŸ¥é¢‘é“æ˜¯å¦æœ‰æ–°è§†é¢‘ï¼ˆä¸å†æŒ‰æ–‡ä»¶ç³»ç»Ÿæ—¶é—´è·³è¿‡ï¼‰
        try:
            # è¯»å–ç¼“å­˜çš„æ•°æ®
            with open(data_filename, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            # è·å–ç¼“å­˜çš„é¢‘é“ID
            channel_id = cached_data.get('channel_id')
            
            if channel_id:
                # è·å–é¢‘é“æœ€æ–°è§†é¢‘ä¿¡æ¯
                playlist_id = f'UU{channel_id[2:]}'
                url = f'https://www.googleapis.com/youtube/v3/playlistItems'
                params = {
                    'part': 'snippet',
                    'playlistId': playlist_id,
                    'maxResults': 1,
                    'key': API_KEY
                }
                
                response = make_api_request(url, params)
                # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢APIå¯†é’¥
                if response is None or response.status_code != 200:
                    API_KEY = try_switch_api_key(API_KEY)
                    params['key'] = API_KEY
                    response = make_api_request(url, params)
                
                if response is not None and response.status_code == 200:
                    latest_data = response.json()
                    if 'items' in latest_data and len(latest_data['items']) > 0:
                        latest_video = latest_data['items'][0]
                        latest_video_id = latest_video['snippet']['resourceId']['videoId']
                        
                        # æ£€æŸ¥æœ€æ–°è§†é¢‘æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
                        cached_video_ids = [video['id'] for video in cached_data.get('videos', []) if 'id' in video]
                        
                        if latest_video_id in cached_video_ids:
                            # æœ€æ–°è§†é¢‘å·²åœ¨ç¼“å­˜ä¸­ï¼Œæ ¹æ®ä¸šåŠ¡ç­–ç•¥å¯é€‰æ‹©è·³è¿‡æˆ–å®šæœŸåˆ·æ–°
                            print(f'é¢‘é“ {info["name"]} æš‚æ— æ–°è§†é¢‘ï¼Œè·³è¿‡å¤„ç†')
                            return
        except Exception as e:
            print(f"æ£€æŸ¥é¢‘é“æ–°è§†é¢‘æ—¶å‡ºé”™: {str(e)}")
            # å‡ºé”™æ—¶ç»§ç»­å¤„ç†ï¼Œä»¥ç¡®ä¿æ•°æ®æ›´æ–°
    
    # è®¡ç®—APIè°ƒç”¨æ¬¡æ•°
    api_calls = (videos_per_channel + 49) // 50  # å‘ä¸Šå–æ•´
    
    # æç¤ºç”¨æˆ·ç¡®è®¤
    if not auto_confirm and videos_per_channel > 100:
        print(f"\nè­¦å‘Š: å³å°†è·å–é¢‘é“ '{info['name']}' çš„ {videos_per_channel} ä¸ªè§†é¢‘")
        print(f"è¿™å°†æ¶ˆè€—çº¦ {api_calls} æ¬¡APIè°ƒç”¨")
        
        confirm = input(f"ç¡®å®šè¦å¤„ç†é¢‘é“ '{info['name']}' å—? (y/n): ").strip().lower()
        if confirm != 'y' and confirm != 'yes':
            print(f"è·³è¿‡å¤„ç†é¢‘é“ '{info['name']}'")
            return
    
    # è·å–é¢‘é“ID
    response = make_api_request(info["url"])
    # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢APIå¯†é’¥
    if response is None or response.status_code != 200:
        print(f"è¯·æ±‚å¤±è´¥: {info['url']}")
        if response is not None:
            print(f"çŠ¶æ€ç : {response.status_code}")
            try:
                error_data = response.json()
                print(f"é”™è¯¯ä¿¡æ¯: {error_data}")
            except:
                print(f"å“åº”å†…å®¹: {response.text[:200]}")
        # ä»URLä¸­æå–å½“å‰APIå¯†é’¥
        current_key = re.search(r'key=([^&]+)', info["url"]).group(1)
        new_key = try_switch_api_key(current_key)
        # æ›´æ–°URLä¸­çš„APIå¯†é’¥
        info["url"] = info["url"].replace(f"key={current_key}", f"key={new_key}")
        print(f"åˆ‡æ¢APIå¯†é’¥å¹¶é‡è¯•...")
        response = make_api_request(info["url"])
    
    if response is not None and response.status_code == 200:
        data = response.json()
        if 'items' in data and len(data['items']) > 0:
            channel_id = data['items'][0]['id']['channelId']
            
            # è·å–è§†é¢‘æ•°æ®
            all_videos, has_new_videos = get_channel_videos_with_limit(channel_id, videos_per_channel)
            
            if all_videos:
                # è¯»å–ç°æœ‰æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                filename = os.path.join(PROJECT_ROOT, 'data', f'{safe_name}.json')
                existing_videos = []
                existing_video_ids = set()
                
                if os.path.exists(filename):
                    try:
                        with open(filename, 'r', encoding='utf-8') as f:
                            existing_data = json.load(f)
                        existing_videos = existing_data.get('videos', [])
                        # å…¼å®¹APIå’ŒRSSä¸¤ç§æ•°æ®æ ¼å¼
                        existing_video_ids = set()
                        for video in existing_videos:
                            video_id = video.get('id', '')  # RSSæ ¼å¼
                            if not video_id:
                                video_id = video.get('snippet', {}).get('resourceId', {}).get('videoId', '')  # APIæ ¼å¼
                            if video_id:
                                existing_video_ids.add(video_id)
                        print(f"ğŸ“ æ‰¾åˆ°ç°æœ‰æ•°æ®ï¼ŒåŒ…å« {len(existing_videos)} ä¸ªè§†é¢‘")
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
                
                # è¿‡æ»¤å‡ºæ–°çš„è§†é¢‘
                new_videos = []
                for video in all_videos:
                    # APIæ–¹å¼è·å–è§†é¢‘ID
                    video_id = video.get('snippet', {}).get('resourceId', {}).get('videoId', '')
                    if video_id and video_id not in existing_video_ids:
                        new_videos.append(video)
                
                print(f"ğŸ†• å‘ç° {len(new_videos)} ä¸ªæ–°è§†é¢‘")
                
                if new_videos or not os.path.exists(filename):
                    # åˆå¹¶æ–°æ—§è§†é¢‘ï¼Œæ–°è§†é¢‘åœ¨å‰
                    all_videos_merged = new_videos + existing_videos
                    
                    # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
                    channel_name = get_channel_info(channel_id)
                    data = {
                        'channel_id': channel_id,
                        'channel_name': channel_name,
                        'original_name': info["name"],
                        'updated_at': datetime.now().isoformat(),
                        'videos': all_videos_merged
                    }
                    
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    
                    if new_videos:
                        print(f'âœ… å¢é‡æ›´æ–°å®Œæˆï¼Œæ€»å…± {len(all_videos_merged)} ä¸ªè§†é¢‘ï¼Œæ–°å¢ {len(new_videos)} ä¸ª')
                    else:
                        print(f'âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}')
                else:
                    print("â„¹ï¸ æ²¡æœ‰æ–°è§†é¢‘ï¼Œæ•°æ®ä¿æŒä¸å˜")
                
                # å¤„ç†æ–‡ä»¶
                try:
                    abs_path = os.path.abspath(filename)
                    result = subprocess.run([sys.executable, os.path.join(SCRIPT_DIR, 'source_processing.py'), abs_path], check=True, capture_output=True, text=True)
                    print(f"æ–‡ä»¶ {abs_path} å¤„ç†å®Œæˆ")
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœæ˜¯å¦æœ‰æ–°å¢è§†é¢‘
                    processing_output = result.stdout
                    processing_has_new_videos = "æ²¡æœ‰æ–°å¢è§†é¢‘" not in processing_output
                    
                    # å¤„ç†å®Œæˆ
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            else:
                print(f'æœªèƒ½è·å–åˆ°é¢‘é“ {info["name"]} çš„è§†é¢‘')
        else:
            print(f'æœªæ‰¾åˆ°é¢‘é“ä¿¡æ¯: {info["name"]}')
    else:
        status_part = f'ï¼ŒçŠ¶æ€ç : {response.status_code}' if response else ''
        print(f'è¯·æ±‚å¤±è´¥{status_part}: {info["url"]}')

# æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥åˆ‡æ¢APIå¯†é’¥
def try_switch_api_key(current_key):
    """
    å½“APIè¯·æ±‚å¤±è´¥æ—¶ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„APIå¯†é’¥
    
    å‚æ•°:
    current_key - å½“å‰ä½¿ç”¨çš„APIå¯†é’¥
    
    è¿”å›:
    æ–°çš„APIå¯†é’¥
    """
    global api_keys
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªAPIå¯†é’¥ï¼Œæ— æ³•åˆ‡æ¢
    if len(api_keys) <= 1:
        print("è­¦å‘Š: æ²¡æœ‰å…¶ä»–APIå¯†é’¥å¯ç”¨")
        return current_key
    
    # æ‰¾åˆ°å½“å‰å¯†é’¥çš„ç´¢å¼•
    try:
        current_index = api_keys.index(current_key)
    except ValueError:
        # å¦‚æœå½“å‰å¯†é’¥ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯†é’¥
        print("å½“å‰APIå¯†é’¥ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯†é’¥")
        return api_keys[0]
    
    # é€‰æ‹©ä¸‹ä¸€ä¸ªå¯†é’¥
    next_index = (current_index + 1) % len(api_keys)
    new_key = api_keys[next_index]
    
    print(f"APIå¯†é’¥å·²åˆ‡æ¢: {current_key[:5]}...{current_key[-3:]} -> {new_key[:5]}...{new_key[-3:]}")
    return new_key

# è·å–æŒ‡å®šæ•°é‡çš„è§†é¢‘
def get_channel_videos_with_limit(channel_id, max_videos=500):
    """è·å–æŒ‡å®šæ•°é‡çš„è§†é¢‘ï¼Œä½¿ç”¨å¢é‡æ›´æ–°ç­–ç•¥"""
    global API_KEY
    
    # è·å–ä¸Šä¼ æ’­æ”¾åˆ—è¡¨ID
    playlist_id = f'UU{channel_id[2:]}'
    url = f'https://www.googleapis.com/youtube/v3/playlistItems'
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç¼“å­˜
    cache_file = None
    cached_videos = []
    cached_video_ids = set()
    
    # æŸ¥æ‰¾è¯¥é¢‘é“çš„ç¼“å­˜æ–‡ä»¶
    data_dir = os.path.join(PROJECT_ROOT, 'data')
    for filename in os.listdir(data_dir):
        if filename.endswith('.json'):
            try:
                with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('channel_id') == channel_id:
                        cache_file = os.path.join(data_dir, filename)
                        cached_videos = data.get('videos', [])
                        cached_video_ids = {v['id'] for v in cached_videos if 'id' in v}
                        break
            except Exception as e:
                print(f"è¯»å–ç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                continue
    
    # æ ‡è®°æ˜¯å¦æœ‰æ–°è§†é¢‘
    has_new_videos = False
    
    # å¢é‡æ›´æ–°ç­–ç•¥
    if cached_videos:
        print(f"æ‰¾åˆ°ç°æœ‰ç¼“å­˜ï¼ŒåŒ…å« {len(cached_videos)} ä¸ªè§†é¢‘")
        
        # åªè·å–æœ€æ–°çš„è§†é¢‘ï¼ˆé€šå¸¸åªéœ€è¦1-2é¡µï¼‰
        new_videos = []
        params = {
            'part': 'snippet',
            'playlistId': playlist_id,
            'maxResults': MAX_RESULTS,
            'key': API_KEY
        }
        
        # æœ€å¤šè·å–3é¡µæ–°è§†é¢‘
        total_api_videos = 0
        for _ in range(3):
            try:
                response = make_api_request(url, params)
                # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢APIå¯†é’¥
                if response is None or response.status_code != 200:
                    API_KEY = try_switch_api_key(API_KEY)
                    params['key'] = API_KEY
                    response = make_api_request(url, params)
                
                if response is not None and response.status_code == 200:
                    data = response.json()
                    videos = data.get('items', [])
                    if not videos:
                        break
                    
                    total_api_videos += len(videos)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°è§†é¢‘
                    all_existing = True
                    for video in videos:
                        # æ·»åŠ é”™è¯¯å¤„ç†
                        if 'snippet' not in video or 'resourceId' not in video.get('snippet', {}):
                            continue
                            
                        video_id = video['snippet']['resourceId']['videoId']
                        if video_id not in cached_video_ids:
                            new_videos.append(video)
                            all_existing = False
                            has_new_videos = True  # æ ‡è®°æœ‰æ–°è§†é¢‘
                    
                    # å¦‚æœè¿™ä¸€é¡µå…¨æ˜¯å·²æœ‰è§†é¢‘ï¼Œå°±ä¸ç”¨ç»§ç»­äº†
                    if all_existing or 'nextPageToken' not in data:
                        break
                    
                    params['pageToken'] = data['nextPageToken']
                else:
                    print(f"APIè¯·æ±‚å¤±è´¥" + (f"ï¼ŒçŠ¶æ€ç : {response.status_code}" if response else ""))
                    break
            except Exception as e:
                print(f"è·å–é¢‘é“è§†é¢‘æ—¶å‡ºé”™: {str(e)}")
                traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
                break
        
        print(f"ä»APIè·å–äº† {total_api_videos} ä¸ªè§†é¢‘ï¼Œå…¶ä¸­ {len(new_videos)} ä¸ªæ˜¯çœŸæ­£çš„æ–°è§†é¢‘")
        
        # å¦‚æœæ²¡æœ‰æ–°è§†é¢‘ï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„è§†é¢‘
        if len(new_videos) == 0:
            print("æ²¡æœ‰æ–°è§†é¢‘ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return cached_videos[:max_videos], False
        
        # åˆå¹¶æ–°æ—§è§†é¢‘ï¼Œä¿æŒæ€»æ•°ä¸è¶…è¿‡max_videos
        # ç¡®ä¿æ–°è§†é¢‘ä¼˜å…ˆä¿ç•™
        all_videos = new_videos.copy()  # å…ˆæ·»åŠ æ‰€æœ‰æ–°è§†é¢‘
        
        # ç„¶åæ·»åŠ æ—§è§†é¢‘ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ•°é‡
        remaining_slots = max_videos - len(all_videos)
        if remaining_slots > 0:
            # éšæœºé€‰æ‹©æ—§è§†é¢‘å¡«å……å‰©ä½™ç©ºé—´
            old_videos_to_keep = cached_videos.copy()
            random.shuffle(old_videos_to_keep)
            
            # æ·»åŠ æ—§è§†é¢‘ï¼Œä½†é¿å…é‡å¤
            for video in old_videos_to_keep:
                if 'snippet' not in video or 'resourceId' not in video.get('snippet', {}):
                    continue
                    
                video_id = video['snippet']['resourceId']['videoId']
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æ–°è§†é¢‘åˆ—è¡¨ä¸­
                if not any(v.get('snippet', {}).get('resourceId', {}).get('videoId') == video_id for v in new_videos):
                    all_videos.append(video)
                    remaining_slots -= 1
                    if remaining_slots <= 0:
                        break
        
        # è¿”å›åˆå¹¶åçš„è§†é¢‘
        if all_videos:
            return all_videos[:max_videos], has_new_videos
        else:
            print("è­¦å‘Š: åˆå¹¶åæ²¡æœ‰æœ‰æ•ˆè§†é¢‘ï¼Œè·³è¿‡åç»­æ­¥éª¤")
            return [], False
    
    # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•è·å–è§†é¢‘
    all_videos = []
    params = {
        'part': 'snippet',
        'playlistId': playlist_id,
        'maxResults': 50,
        'key': API_KEY
    }
    
    # è®¡ç®—éœ€è¦è·å–çš„é¡µæ•°ï¼ˆé™åˆ¶æœ€å¤§é¡µæ•°ä¸º20ï¼Œçº¦1000ä¸ªè§†é¢‘ï¼‰
    pages_needed = min(20, (max_videos + 49) // 50)  # å‘ä¸Šå–æ•´ï¼Œä½†æœ€å¤š20é¡µ
    
    total_api_videos = 0
    for _ in range(pages_needed):
        try:
            response = make_api_request(url, params)
            # å¦‚æœAPIè¯·æ±‚å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢APIå¯†é’¥
            if response is None or response.status_code != 200:
                API_KEY = try_switch_api_key(API_KEY)
                params['key'] = API_KEY
                response = make_api_request(url, params)
            
            if response is not None and response.status_code == 200:
                data = response.json()
                videos = data.get('items', [])
                if not videos:
                    break
                
                total_api_videos += len(videos)
                    
                # è¿‡æ»¤æ‰æ— æ•ˆçš„è§†é¢‘
                valid_videos = [v for v in videos if 'snippet' in v and 'resourceId' in v.get('snippet', {})]
                all_videos.extend(valid_videos)
                
                if 'nextPageToken' not in data or len(all_videos) >= max_videos:
                    break
                    
                params['pageToken'] = data['nextPageToken']
            else:
                print(f"APIè¯·æ±‚å¤±è´¥" + (f"ï¼ŒçŠ¶æ€ç : {response.status_code}" if response else ""))
                break
        except Exception as e:
            print(f"è·å–é¢‘é“è§†é¢‘æ—¶å‡ºé”™: {str(e)}")
            traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            break
    
    print(f"ä»APIè·å–äº† {total_api_videos} ä¸ªè§†é¢‘")
    
    # éšæœºæ‰“ä¹±å¹¶è¿”å›
    if all_videos:
        random.shuffle(all_videos)
        return all_videos[:max_videos], True  # é¦–æ¬¡è·å–è§†é¢‘ï¼Œè®¤ä¸ºéƒ½æ˜¯æ–°çš„
    else:
        print(f"è­¦å‘Š: æœªèƒ½è·å–åˆ°ä»»ä½•æœ‰æ•ˆè§†é¢‘")
        return [], False

# æ‰§è¡Œsource_processing.py
def process_source_files():
    try:
        print("å¼€å§‹æ‰§è¡Œsource_processing.py...")
        subprocess.run([sys.executable, os.path.join(SCRIPT_DIR, 'source_processing.py')], check=True)
        print("source_processing.pyæ‰§è¡Œå®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"æ‰§è¡Œsource_processing.pyæ—¶å‡ºé”™: {e}")
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        traceback.print_exc()
        
# ä¿®æ”¹mainå‡½æ•°ï¼Œç§»é™¤only_uncachedå‚æ•°
def main(force_update=False, auto_task=False, videos_per_channel=500):
    # è·å–æ‰€æœ‰é¢‘é“
    channels_to_process = shuffled_info.copy()
    
    # å¦‚æœæ˜¯è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºé¢‘é“
    if auto_task:
        # æ£€æŸ¥æ¯ä¸ªé¢‘é“çš„ç¼“å­˜çŠ¶æ€
        cached_channels = []
        
        for info in channels_to_process:
            data_filename = f'../data/{info["name"]}.json'
            if os.path.exists(data_filename):
                # å·²ç¼“å­˜çš„é¢‘é“ï¼ŒæŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
                file_mtime = os.path.getmtime(data_filename)
                cached_channels.append((info, file_mtime))
            else:
                # å¦‚æœæœ‰æœªç¼“å­˜çš„é¢‘é“ï¼ˆä¸å¤ªå¯èƒ½ï¼‰ï¼Œæ”¾åœ¨æœ€å‰é¢
                cached_channels.insert(0, (info, 0))
        
        # å¯¹å·²ç¼“å­˜é¢‘é“æŒ‰æ›´æ–°æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„ä¼˜å…ˆï¼‰
        cached_channels.sort(key=lambda x: x[1])
        channels_to_process = [item[0] for item in cached_channels]
        
        print(f"è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼: æ‰¾åˆ° {len(cached_channels)} ä¸ªé¢‘é“ï¼ŒæŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº")
    
    # åˆ†ç»„å¤„ç†é¢‘é“
    total_channels = len(channels_to_process)
    channels_per_group = (total_channels + len(api_keys) - 1) // len(api_keys)
    
    print(f"æ€»å…±æœ‰ {total_channels} ä¸ªé¢‘é“ï¼Œåˆ†æˆ {len(api_keys)} ç»„ï¼Œæ¯ç»„çº¦ {channels_per_group} ä¸ªé¢‘é“")
    
    # è®¡ç®—æ¯ä¸ªAPIå¯†é’¥å¯ä»¥å¤„ç†çš„é¢‘é“æ•°é‡ï¼ˆè€ƒè™‘é…é¢é™åˆ¶ï¼‰
    # å‡è®¾æ¯ä¸ªé¢‘é“éœ€è¦ (videos_per_channel/50) æ¬¡APIè°ƒç”¨ï¼Œæ¯ä¸ªå¯†é’¥æ¯å¤©æœ‰10,000å•ä½é…é¢
    # ä¸ºå®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬åªä½¿ç”¨é…é¢çš„80%
    api_calls_per_channel = (videos_per_channel + 49) // 50
    safe_quota_per_key = 8000  # 80% of 10,000
    max_channels_per_key = safe_quota_per_key // api_calls_per_channel
    
    if auto_task:
        print(f"è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼: æ¯ä¸ªé¢‘é“éœ€è¦çº¦ {api_calls_per_channel} æ¬¡APIè°ƒç”¨")
        print(f"æ¯ä¸ªAPIå¯†é’¥å®‰å…¨é…é¢ä¸º {safe_quota_per_key} å•ä½ï¼Œå¯ä»¥å¤„ç†çº¦ {max_channels_per_key} ä¸ªé¢‘é“")
    
    # å¤„ç†é¢‘é“
    channels_processed = 0
    for i, api_key in enumerate(api_keys):
        if channels_processed >= total_channels:
            break
            
        # åœ¨è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ä¸‹ï¼Œé™åˆ¶æ¯ä¸ªå¯†é’¥å¤„ç†çš„é¢‘é“æ•°é‡
        channels_for_this_key = min(
            max_channels_per_key if auto_task else float('inf'),
            channels_per_group,
            total_channels - channels_processed
        )
        
        if channels_for_this_key <= 0:
            continue
            
        start_idx = channels_processed
        end_idx = channels_processed + channels_for_this_key
        
        group_channels = channels_to_process[start_idx:end_idx]
        print(f"\n=== å¤„ç†ç¬¬ {i+1} ç»„é¢‘é“ (ä½¿ç”¨APIå¯†é’¥ {api_key[:5]}...{api_key[-3:]}) ===")
        print(f"æœ¬ç»„åŒ…å« {len(group_channels)} ä¸ªé¢‘é“")
        
        # ä¸ºè¿™ç»„é¢‘é“è®¾ç½®å…¨å±€APIå¯†é’¥
        global API_KEY
        API_KEY = api_key
        
        # å¤„ç†è¿™ç»„é¢‘é“
        for info in group_channels:
            try:
                # åœ¨è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ä¸‹è‡ªåŠ¨ç¡®è®¤
                process_channel(info, videos_per_channel, auto_confirm=auto_task)
                channels_processed += 1
            except Exception as e:
                print(f"å¤„ç†é¢‘é“ {info['name']} æ—¶å‡ºé”™: {str(e)}")
                continue
    
    print(f"\næ€»å…±å¤„ç†äº† {channels_processed} ä¸ªé¢‘é“")
    
    # å¤„ç†æºæ–‡ä»¶
    process_source_files()

if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='å¤„ç†YouTubeé¢‘é“è§†é¢‘æ•°æ®')
    parser.add_argument('--force', '-f', action='store_true',
                      help='å¼ºåˆ¶æ›´æ–°æ‰€æœ‰é¢‘é“ï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥')
    parser.add_argument('--videos-per-channel', type=int, default=500,
                      help='æ¯ä¸ªé¢‘é“è·å–çš„è§†é¢‘æ•°é‡ï¼Œé»˜è®¤500')
    parser.add_argument('--yes', '-y', action='store_true',
                      help='è‡ªåŠ¨ç¡®è®¤æ‰€æœ‰æç¤ºï¼Œä¸è¯¢é—®ç”¨æˆ·')
    parser.add_argument('--auto-task', action='store_true',
                      help='è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ï¼Œä¼˜å…ˆå¤„ç†æœªç¼“å­˜é¢‘é“ï¼Œè‡ªåŠ¨ç®¡ç†APIé…é¢')
    args = parser.parse_args()
    
    # å½“è¯·æ±‚å¤§é‡è§†é¢‘æ—¶æç¤ºç”¨æˆ·ç¡®è®¤ï¼ˆé™¤éæ˜¯è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼ï¼‰
    if args.videos_per_channel > 100 and not args.yes and not args.auto_task:
        # è®¡ç®—APIè°ƒç”¨æ¬¡æ•°å’Œæ€»é¢‘é“æ•°
        api_calls_per_channel = (args.videos_per_channel + 49) // 50  # å‘ä¸Šå–æ•´
        total_channels = len(shuffled_info)
        total_api_calls = api_calls_per_channel * total_channels
        print(f"\nè­¦å‘Š: æ‚¨æ­£åœ¨è¯·æ±‚æ¯ä¸ªé¢‘é“è·å– {args.videos_per_channel} ä¸ªè§†é¢‘")
        print(f"è¿™å°†æ¶ˆè€—å¤§é‡APIé…é¢:")
        print(f"- æ¯ä¸ªé¢‘é“çº¦éœ€ {api_calls_per_channel} æ¬¡APIè°ƒç”¨")
        print(f"- æ€»å…±æœ‰ {total_channels} ä¸ªé¢‘é“")
        print(f"- é¢„è®¡æ€»å…±éœ€è¦ {total_api_calls} æ¬¡APIè°ƒç”¨")
        print(f"- æ‚¨æœ‰ {len(api_keys)} ä¸ªAPIå¯†é’¥ï¼Œæ¯ä¸ªå¯†é’¥æ¯å¤©é…é¢çº¦10,000å•ä½")
        
        confirm = input("\nç¡®å®šè¦ç»§ç»­æ•´ä¸ªå¤„ç†è¿‡ç¨‹å—? (y/n): ").strip().lower()
        if confirm != 'y' and confirm != 'yes':
            print("æ“ä½œå·²å–æ¶ˆ")
            exit(0)
    
    # è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼
    if args.auto_task:
        main(args.force, True, args.videos_per_channel)
    # ä½¿ç”¨è‡ªå®šä¹‰è§†é¢‘æ•°é‡
    elif args.videos_per_channel:
        for info in shuffled_info:
            try:
                process_channel(info, args.videos_per_channel, args.yes)
            except Exception as e:
                print(f"å¤„ç†é¢‘é“ {info['name']} æ—¶å‡ºé”™: {str(e)}")
        process_source_files()
    # ä½¿ç”¨åŸæœ‰é€»è¾‘
    else:
        main(args.force, False, 500)