#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„é¢‘é“è§†é¢‘è·å–è„šæœ¬ - å…ˆå»uploadåŠŸèƒ½
"""

import json
import os
import urllib.request
import urllib.parse
import urllib.error
import xml.etree.ElementTree as ET
from datetime import datetime
import argparse
import time
import sys

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def fetch_channel_videos_via_rss(channel_id, max_count=200):
    """ä½¿ç”¨RSSæ–¹å¼è·å–é¢‘é“è§†é¢‘åˆ—è¡¨"""
    if not channel_id:
        return []
    
    feed_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        req = urllib.request.Request(feed_url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            xml_text = resp.read()
    except Exception as e:
        print(f"âŒ RSSè·å–å¤±è´¥: {e}")
        return []

    try:
        root = ET.fromstring(xml_text)
    except Exception as e:
        print(f"âŒ RSSè§£æå¤±è´¥: {e}")
        return []

    ns = {
        'atom': 'http://www.w3.org/2005/Atom',
        'media': 'http://search.yahoo.com/mrss/'
    }
    entries = root.findall('atom:entry', ns)

    videos = []
    for entry in entries[:max_count]:
        # å®‰å…¨åœ°è·å–video_id
        video_id = None
        try:
            video_id_el = entry.find('yt:videoId', {'yt': 'http://www.youtube.com/xml/schemas/2015'})
            if video_id_el is not None and hasattr(video_id_el, 'text'):
                video_id = video_id_el.text
        except Exception:
            pass
        
        if not video_id:
            # å¤‡ç”¨ï¼šä» link href ä¸­è§£æ v å‚æ•°
            try:
                link_el = entry.find('atom:link', ns)
                if link_el is not None:
                    href = link_el.get('href', '')
                    q = urllib.parse.urlparse(href).query
                    qs = urllib.parse.parse_qs(q)
                    video_id = (qs.get('v') or [''])[0]
            except Exception:
                pass

        # å®‰å…¨åœ°è·å–æ ‡é¢˜
        title = ""
        try:
            title_el = entry.find('atom:title', ns)
            if title_el is not None and hasattr(title_el, 'text'):
                title = title_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–å‘å¸ƒæ—¶é—´
        published_at = ""
        try:
            published_el = entry.find('atom:published', ns)
            if published_el is not None and hasattr(published_el, 'text'):
                published_at = published_el.text
        except Exception:
            pass

        # å®‰å…¨åœ°è·å–ç¼©ç•¥å›¾
        thumb_url = None
        try:
            media_group = entry.find('media:group', ns)
            if media_group is not None:
                thumb = media_group.find('media:thumbnail', ns)
                if thumb is not None:
                    thumb_url = thumb.get('url')
        except Exception:
            pass

        videos.append({
            "id": video_id or "",
            "title": title,
            "description": "",
            "publishedAt": published_at,
            "thumbnails": {"default": {"url": thumb_url}} if thumb_url else {},
            "url": f"https://www.youtube.com/watch?v={video_id}" if video_id else ""
        })

    return videos


def get_channel_id_from_url(url):
    """ä»YouTube URLè·å–é¢‘é“ID"""
    if not url:
        return None
    
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=30) as resp:
            html = resp.read().decode("utf-8", errors="ignore")
        
        # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾channelId
        patterns = [
            r'"channelId"\s*:\s*"(UC[\w-]{22})"',
            r'"externalId"\s*:\s*"(UC[\w-]{22})"',
            r'"ucid"\s*:\s*"(UC[\w-]{22})"',
            r'"channelId":"(UC[\w-]{22})"',
            r'channelId.*?"(UC[\w-]{22})"',
        ]
        
        import re
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
    except Exception as e:
        print(f"âŒ æ— æ³•ä»URLè·å–é¢‘é“ID: {e}")
    
    return None


def process_channel_rss(info):
    """ä½¿ç”¨RSSæ–¹å¼å¤„ç†é¢‘é“ï¼Œå®ç°å¢é‡æ›´æ–°"""
    print(f'\n=== RSSæ–¹å¼å¤„ç†é¢‘é“: {info["name"]} ===')
    
    # è·å–é¢‘é“ID
    channel_id = None
    if info.get("url"):
        channel_id = get_channel_id_from_url(info["url"])
    
    # å¦‚æœä»ç¼“å­˜æ–‡ä»¶è·å–é¢‘é“ID
    safe_name = info.get("bakname", "").strip()
    if not safe_name:
        safe_name = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in info["name"])
    
    data_filename = os.path.join(PROJECT_ROOT, 'data', f'{safe_name}.json')
    
    # è¯»å–ç°æœ‰æ•°æ®
    existing_data = None
    existing_videos = []
    existing_video_ids = set()
    
    if os.path.exists(data_filename):
        try:
            with open(data_filename, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            existing_videos = existing_data.get('videos', [])
            existing_video_ids = {video.get('id', '') for video in existing_videos if video.get('id')}
            channel_id = existing_data.get('channel_id') or channel_id
            print(f"ğŸ“ æ‰¾åˆ°ç°æœ‰æ•°æ®æ–‡ä»¶ï¼ŒåŒ…å« {len(existing_videos)} ä¸ªè§†é¢‘")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    
    if not channel_id:
        print(f"âš ï¸ æ— æ³•è·å–é¢‘é“IDï¼Œè·³è¿‡: {info['name']}")
        return False
    
    print(f"âœ… æ‰¾åˆ°é¢‘é“ID: {channel_id}")
    
    # ä½¿ç”¨RSSè·å–æœ€æ–°è§†é¢‘
    rss_videos = fetch_channel_videos_via_rss(channel_id, max_count=200)
    if rss_videos:
        print(f"âœ… RSSè·å–åˆ° {len(rss_videos)} ä¸ªè§†é¢‘")
        
        # è¿‡æ»¤å‡ºæ–°çš„è§†é¢‘
        new_videos = []
        for video in rss_videos:
            video_id = video.get('id', '')
            if video_id and video_id not in existing_video_ids:
                new_videos.append(video)
        
        print(f"ğŸ†• å‘ç° {len(new_videos)} ä¸ªæ–°è§†é¢‘")
        
        if new_videos:
            # åˆå¹¶æ–°æ—§è§†é¢‘ï¼Œæ–°è§†é¢‘åœ¨å‰
            all_videos = new_videos + existing_videos
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            rss_data = {
                "channel_id": channel_id,
                "channel_name": info["name"],
                "updated_at": datetime.now().isoformat(),
                "videos": all_videos,
            }
            
            with open(data_filename, 'w', encoding='utf-8') as f:
                json.dump(rss_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å¢é‡æ›´æ–°å®Œæˆï¼Œæ€»å…± {len(all_videos)} ä¸ªè§†é¢‘ï¼Œæ–°å¢ {len(new_videos)} ä¸ª")
        else:
            print("â„¹ï¸ æ²¡æœ‰æ–°è§†é¢‘ï¼Œæ•°æ®ä¿æŒä¸å˜")
        
        return True
    else:
        print("âš ï¸ RSSæœªè·å–åˆ°è§†é¢‘æ•°æ®")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä½¿ç”¨RSSæ–¹å¼æ›´æ–°é¢‘é“è§†é¢‘æ•°æ®')
    parser.add_argument('--auto-task', action='store_true', help='è‡ªåŠ¨ä»»åŠ¡æ¨¡å¼')
    args = parser.parse_args()
    
    # è¯»å–é¢‘é“é…ç½®
    with open(os.path.join(PROJECT_ROOT, 'all_channels.json'), 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–æ‰€æœ‰é¢‘é“
    all_channels = []
    for category, subcategories in data.items():
        if isinstance(subcategories, dict):
            for subcategory, channels in subcategories.items():
                if isinstance(channels, list):
                    for channel in channels:
                        if isinstance(channel, dict) and channel.get("url") and not channel.get("skip"):
                            all_channels.append(channel)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_channels)} ä¸ªé¢‘é“")
    
    # å¤„ç†é¢‘é“
    success_count = 0
    for i, channel in enumerate(all_channels, 1):
        print(f"\n[{i}/{len(all_channels)}] å¤„ç†é¢‘é“: {channel['name']}")
        try:
            if process_channel_rss(channel):
                success_count += 1
        except Exception as e:
            print(f"âŒ å¤„ç†é¢‘é“ {channel['name']} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(all_channels)} ä¸ªé¢‘é“")


if __name__ == "__main__":
    main()
